{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fd1bc41",
   "metadata": {},
   "source": [
    "# Using TensorFlow to create and train neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bdc8d6",
   "metadata": {},
   "source": [
    "To work with TensorFlow, we first need to install it. Uncomment the code below, if you haven't already done so (note that we use `pydot` and `graphviz` for visualizing the hidden layers of a neural network - but they are not necessary to run neural networks in TensorFlow).\n",
    "\n",
    "Note that parts 4 and 6 of the notebook build on a GitHub repository by [Aurélien Géron](https://github.com/ageron/handson-ml2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665647a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow\n",
    "#pip install pydot\n",
    "#pip install graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fcb906",
   "metadata": {},
   "source": [
    "Once we have installed all packages, let's do the obligatory imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21cd366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f570d847",
   "metadata": {},
   "source": [
    "# 1. Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bc5cad",
   "metadata": {},
   "source": [
    "## 1.1 Creating tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00797399",
   "metadata": {},
   "source": [
    "There are two types of \"variables\" in TensorFlow that you might work with. Actual variables, `tf.Variables`, which can be changed once they have been set, as well as constants , `tf.constant`, which cannot be changed anymore. Let's create a random array of data `X`, as well as parameter matrices `W` and `b`. Can you see why we would use `tf.constant` for `X` and `tf.Variable` for `W` and `b`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cda2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.constant(np.random.randn(100,5), name = \"X\")\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723fc939",
   "metadata": {},
   "source": [
    "Note that the \"tensor\" we create has exactly the dimension of the numpy array we use to initialize it. TensorFlow allows you to use `numpy` data quite seamlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bece0cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(np.random.randn(5,3), name = \"W\")\n",
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132289fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.Variable(np.random.randn(1,3), name = \"b\")\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c431a734",
   "metadata": {},
   "source": [
    "## 1.2 Basic operations on tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbe51e1",
   "metadata": {},
   "source": [
    "Let's now perform some basic operations on tensors. These work essentially the same way as in `numpy` (e.g., the typical rules of matrix-multiplication apply when we use `tf.matmul`), just that we have to learn a few different key-words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78743e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = tf.add(tf.matmul(X,W),b)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb3c1e6",
   "metadata": {},
   "source": [
    "TensorFlow has all the standard activation functions in-built, for example, the logistic sigmoid one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49214ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.keras.activations.sigmoid(z)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8814521",
   "metadata": {},
   "source": [
    "## 1.3 Type-casting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcb5987",
   "metadata": {},
   "source": [
    "One aspect of TensorFlow you might know from other programming languages, but which is usually taken care of for you in base Python, is that types are not automatically \"cast\". What that means is that we cannot run operations such as multiplying an integer and a float tensor. We cannot even multiply a `float32` tensor with a `float64` tensor! You can find the datatype of any tensor by printing it out, or by specifically looking at `tensor.dtype`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de43641e",
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377afca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72b18a1",
   "metadata": {},
   "source": [
    "We will now (explicitly) convert `b` from a `tf.float64` data-type (a float with 64 bits of precision), to a `tf.float32` data-type (a float with 32 bits of precision):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4223b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.cast(b,tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adab1496",
   "metadata": {},
   "source": [
    "Note that `X` and `W` are still of type `tf.float64` (the default). Let's try the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104118bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.add(tf.matmul(X,W),b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb344ff",
   "metadata": {},
   "source": [
    "This didn't work! Let's re-cast `b` into a `tf.float64`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e977ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.cast(b, tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6521f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7aeb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.add(tf.matmul(X,W),b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bca134",
   "metadata": {},
   "source": [
    "Now it works again!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcd0bf7",
   "metadata": {},
   "source": [
    "## 1.4 Some other useful operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b864790",
   "metadata": {},
   "source": [
    "Let's look at `tf.one_hot` and `tf.reshape`, two operations you come across a lot when handling data in TensorFlow. Can you see what they do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fd7423",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.constant(3)\n",
    "y_one_hot = tf.one_hot(y,4,axis=0)\n",
    "y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ff9e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [2]\n",
    "y_one_hot = tf.one_hot(y,4,axis=0)\n",
    "y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debe30d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reshape(y_one_hot,(4,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ef7259",
   "metadata": {},
   "source": [
    "# 2. Creating a simple neural network with the Sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f106c9f2",
   "metadata": {},
   "source": [
    "We will recreate what we did in the last lecture: a logistic regression model that allows us to differentiate between sign-language digits 0 and 1. But this time, we will be using TensorFlow's Sequential API to build a (single-neuron) neural network. The data-handling is the same as last week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d88bf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('digits_X.npy')\n",
    "y = np.load('digits_y.npy')\n",
    "X = np.concatenate((X[204:409], X[822:1028] ), axis=0)\n",
    "z = np.zeros(409-204)\n",
    "o = np.ones(1028-822)\n",
    "y = np.concatenate((z, o), axis=0).reshape(X.shape[0],1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=172)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20a52f6",
   "metadata": {},
   "source": [
    "## 2.1 Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d47f43",
   "metadata": {},
   "source": [
    "Let's create the model. Notice that we haven't flattened the observations (recall that each value of `X` is a matrix!). The beauty about TensorFlow is that we can simply add in a `Flatten` layer, to do this for us. Hence, we have two layers in our model: the `Flatten` layer (not an actual neural network layer), and a standard `Dense` layer, with one neuron/unit (the one that performs the logistic regression).\n",
    "\n",
    "For our `Dense` layer with one neuron, we also need to specify the activation function. Of course, for a logistic regression, we use the logistic sigmoid function, simply `'sigmoid'` in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c324809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(64, 64)),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cc1e38",
   "metadata": {},
   "source": [
    "Now that we have created our model, we can take a look at its layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95977520",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07c5676",
   "metadata": {},
   "source": [
    "The `model.summary()` is a bit more informative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafa8e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d645a0",
   "metadata": {},
   "source": [
    "The `Flatten` layer flattens each input from a (64,64)-matrix to a (4096,)-vector. However, it says (None, 4096) for the output shape. Can you guess why?\n",
    "\n",
    "Also, can you guess what the 4097 parameters are?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f182a0",
   "metadata": {},
   "source": [
    "In order to use our model, we also have to `compile` it. This is the step where we decide on\n",
    "- the optimization algorithm (we will use `'sgd'`, or \"stochastic gradient descent\", which is very similar to the gradient descent method we've seen before - we will deal with the exact difference later),\n",
    "- the loss function (we will use `'binary_crossentropy'`, as before),\n",
    "- the metrics to keep track of as the model is being trained, in the form of a list (we will only keep track of `'accuracy'` for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a29ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b81170",
   "metadata": {},
   "source": [
    "That's it! When we compile the model, it automatically creates the forward propagation and back-propagation steps for us, based on the layers and cost function we define!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3acf7c",
   "metadata": {},
   "source": [
    "## 2.2 Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fea9f7",
   "metadata": {},
   "source": [
    "We will now train the model, using the training data. We will run it for 100 \"epochs\", that is iterations of our optimization algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ae018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f5e545",
   "metadata": {},
   "source": [
    "Let's restart the process (we will use `clear_session` to properly clear out TensorFlow's cache, and then define random seeds for `numpy` and `tf` (this is good practice to do right after the clear out). Other than that, we will only make three small changes:\n",
    "- We add validation data to the training process (we will here simply use the test data - not best practice, but this is just for showing you how the TensorFlow training can directly consider a validation data set)\n",
    "- Instead of using the default `'sgd'` optimizer, we manually specify the learning rate, using the `tf.keras.optimizers.SGD` object\n",
    "- We also save the output of `model.fit`, as this will be interesting for drawing graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1877267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "np.random.seed(672)\n",
    "tf.random.set_seed(323)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5406c3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(64, 64)),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "log = model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17af278",
   "metadata": {},
   "source": [
    "We can now use the log to see how the training went. The `log.history` is a dictionary that will capture, for each epoch, all the metrics we defined for our model (`['accuracy']` in our case), as well as the cost/loss, for both the training and the \"validation\" dataset. The keywords to use in the dictionary are simply the words you can find in the log above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821b4198",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(log.history['accuracy'],label = \"training accuracy\")\n",
    "plt.plot(log.history['val_accuracy'], label = \"testing accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d7aaf8",
   "metadata": {},
   "source": [
    "We can also look at the parameters and how they have been adjusted when the model was trained. Remember that `model.layers` gives us all the layers of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebb715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3e6597",
   "metadata": {},
   "source": [
    "We can access these layers with standard Pythonic indexing. E.g., the output layer in our case is layer 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05b2712",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = model.layers[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacd5708",
   "metadata": {},
   "source": [
    "Let's now take a look at both the weights and the bias after training. Before you run the cell, ask yourself: how many weights and biases do you expect there to be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bd5348",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, bias = output_layer.get_weights()\n",
    "print(weights)\n",
    "print('----')\n",
    "print(weights.shape)\n",
    "print('----')\n",
    "print(bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5ab15e",
   "metadata": {},
   "source": [
    "## 2.3 Making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3e1976",
   "metadata": {},
   "source": [
    "To make predictions, we simply input the data we want to make predictions on into the model we have trained. `.numpy()` just helps with formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8eece6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(X_test).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b5c531",
   "metadata": {},
   "source": [
    "As before, our predictions are values between 0 and 1. let's get actual binary classification predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfdfd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa63c5a",
   "metadata": {},
   "source": [
    "# 3. Creating a deeper neural network with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839802a5",
   "metadata": {},
   "source": [
    "We will now create a deeper neural network with TensorFlow's Sequential API (the same one we were just using now). Before we do so, let's properly clear out the cache and set some new random seeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99178380",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "np.random.seed(745)\n",
    "tf.random.set_seed(498)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27922aa2",
   "metadata": {},
   "source": [
    "We can now create the model. We can either create it in one go:\n",
    "```\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(64, 64)),\n",
    "  tf.keras.layers.Dense(3, kernel_initializer = 'uniform', activation='relu')\n",
    "  tf.keras.layers.Dense(3, kernel_initializer = 'uniform', activation='relu')\n",
    "  tf.keras.layers.Dense(1, kernel_initializer = 'uniform', activation='sigmoid')\n",
    "])\n",
    "```\n",
    "or we can add layers one-by-one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bd0ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=(64,64)))\n",
    "model.add(tf.keras.layers.Dense(3, kernel_initializer = 'uniform', activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(3, kernel_initializer = 'uniform', activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(1, kernel_initializer = 'uniform', activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8a1d9a",
   "metadata": {},
   "source": [
    "A few things to note: We are using the `Flatten` layer again, to convert our input-matrices into input-vectors. We then add a number of hidden layers and an output layer. Can you see how many hidden layers there are and how many neurons are at each?\n",
    "\n",
    "As in the tutorial, we use `'relu'` units at the hidden layer and a `sigmoid` unit at the output layer. We also add in a `kernel_initializer`: we are specifying how the parameters should be initialized (uniformly random). In this particular case, the default initialization is likely not to work, but feel free to try it out as well.\n",
    "\n",
    "In general, if you are unsure about any of the inputs to the TensorFlow functions, make sure to check the [amazing documentation](https://www.tensorflow.org/api_docs/python/tf/all_symbols).\n",
    "\n",
    "As before, we can print out a summary of the `model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912c9825",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabfa407",
   "metadata": {},
   "source": [
    "Do the shapes make sense to you?\n",
    "\n",
    "Can you account for the number of parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd73e0b2",
   "metadata": {},
   "source": [
    "We can also visualize the layers of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e3a591",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bae996",
   "metadata": {},
   "source": [
    "And as before, we can take a look at it's parameters (note that we haven't trained the model yet, so these are the randomly generated weights and biases):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7afbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer = model.layers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1022d865",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, bias = hidden_layer.get_weights()\n",
    "print(weights)\n",
    "print('----')\n",
    "print(weights.shape)\n",
    "print('----')\n",
    "print(bias)\n",
    "print('----')\n",
    "print(bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced9387c",
   "metadata": {},
   "source": [
    "Finally, before training the model, we need to compile it again. As before, we use `'sgd'` as an optimization algorithm, and `'binary_crossentropy'` as our loss (or cost) function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137e6c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbd0056",
   "metadata": {},
   "source": [
    "## 3.1 Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e078a1",
   "metadata": {},
   "source": [
    "Let's now run the model for 100 epochs, similar to what we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b16b599",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = model.fit(X_train, y_train, epochs=100,validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc72d6f",
   "metadata": {},
   "source": [
    "Did you see anything happening? Why do you think this is the case? What do you think happens if we really crank up the iterations (say, to a few thousand)?\n",
    "\n",
    "\n",
    "\n",
    "We will try again, this time with a different optimization-algorithm, `'adam'` (again, don't worry too much about what exactly `'adam'` does, we will see this later in class). For this, we have to recompile the model. Keep in mind, however, that the weights and biases will already be at exactly the level they have been trained to in the previous training attempt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9abc175",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659be2a7",
   "metadata": {},
   "source": [
    "Let's try again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b574b7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = model.fit(X_train, y_train, epochs=100,validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15cecdc",
   "metadata": {},
   "source": [
    "Looks much better, right? As before, we can display the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19e927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(log.history['accuracy'],label = \"training accuracy\")\n",
    "plt.plot(log.history['val_accuracy'], label = \"testing accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ba9e9e",
   "metadata": {},
   "source": [
    "# 4. Can you build on what you have seen? Creating a neural network for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38524b01",
   "metadata": {},
   "source": [
    "We will now use a neural network for regression instead of classification. In particular, we will load a standard scikit-learn dataset that contains key characteristics of some houses in california, as well as their values (in $100,000). We will then try to predict the houses' values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aacb91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773f46c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb1fb09",
   "metadata": {},
   "source": [
    "The housing data comes as a dictionary: We can find the features of the data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1b83ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing['feature_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fec7d8",
   "metadata": {},
   "source": [
    "The actual observations (or, more precisely, the feature matrix `X`) comes with the key `'data'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557acbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing['data'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990ae1bd",
   "metadata": {},
   "source": [
    "The labels come with the key `'target'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8211122b",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing['target'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d43b4b",
   "metadata": {},
   "source": [
    "As always, we start by splitting the data. We will use a training, a validation, and a testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bcb1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_other, y_train, y_other = train_test_split(housing['data'], housing['target'], train_size=0.7, random_state=461)\n",
    "X_test, X_valid, y_test, y_valid = train_test_split(X_other, y_other, train_size = 0.5, random_state=391)\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0b7d3f",
   "metadata": {},
   "source": [
    "Next, we normalize the X-data. Keep in mind to not use the validation or test data in defining how the scaling process works (otherwise, you can leak information about this data into your training process). You can, however, use the same scaler on validation and test data, rather than defining your own scalers here - this will usually work better than normalizing all data independently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d5e817",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724ad403",
   "metadata": {},
   "source": [
    "## 4.1 Building a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df214576",
   "metadata": {},
   "source": [
    "Let's clear our TensorFlow session and set new seeds. Even if not strictly necessary, this is good practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d85d8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "np.random.seed(231)\n",
    "tf.random.set_seed(631)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6395f9",
   "metadata": {},
   "source": [
    "Now it's your turn! Build a neural network with three `Dense` layers: two hidden layers with `'relu'` activation, and one final output layer. The hidden layers should have 10 neurons each. For the first hidden layer, you have to specify the correct `input_shape`. Recall that this corresponds to the columns in our X-data. But make sure that `input_shape` is a list or a tuple, not just a single value, otherwise there will be an error.\n",
    "\n",
    "For the output layer, consider whether we need any activation function at all. Remember that a linear regression is just a weighted sum of inputs plus a bias (sounds familiar?)\n",
    "\n",
    "Save your model as `model`.\n",
    "\n",
    "If you are stuck, first take a look at the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) of the `Dense` layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9637d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49312a01",
   "metadata": {},
   "source": [
    "As always, it helps to visualize your model with the `summary()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa065c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2be969bd",
   "metadata": {},
   "source": [
    "As before, try to answer the following questions:\n",
    "- Do the shapes make sense to you?\n",
    "- Can you account for the number of parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cc7344",
   "metadata": {},
   "source": [
    "Now, compile your model. We are doing regression, so it is quite natural to use `'mean_squared_error'` as the loss function.\n",
    "\n",
    "Moreover, we will us `'sgd'` as before. However, instead of specifying `optimizer = 'sgd'`, we will make a custom optimizer function, which allows us to choose our own learning rate. In particular, you should use\n",
    "```\n",
    "optimizer=tf.keras.optimizers.SGD(learning_rate=0.03))\n",
    "```\n",
    "\n",
    "Finally, since we are not doing classification, there is no accuracy to keep track of, so simply leave out the `metrics` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275d60f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1356000",
   "metadata": {},
   "source": [
    "Train your model for 30 epochs, also keeping track of the `validation_data`. Save the output as `log`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf352b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7690934d",
   "metadata": {},
   "source": [
    "Use the code below to display the loss on both the training and the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1d43aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(log.history['loss'],label = \"training loss\")\n",
    "plt.plot(log.history['val_loss'], label = \"validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c7a0ba",
   "metadata": {},
   "source": [
    "We can evaluate the model on the test set directly, using `model.evaluate(X_test,y_test)`. Try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3dc64e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2360f4d6",
   "metadata": {},
   "source": [
    "Finally, we can take a look at some examples and see how well we are doing on those. Run the code below, and feel free to adjust the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87250279",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_test[:5]\n",
    "y_predict = model.predict(X_new)\n",
    "y_predict = y_predict.reshape(-1,)\n",
    "y_predict = [round(y,3) for y in y_predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3896ab2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicted values: \" + str(y_predict))\n",
    "print(\"True values: \" + str(y_test[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36adc085",
   "metadata": {},
   "source": [
    "Can you do better? Try tweaking the learning rate, the number of layers, and the neurons per layer to see if your validation loss improves. Once you have decided on your final model, evaluate it on the test set and note down your loss there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1103ea",
   "metadata": {},
   "source": [
    "## 4.2 Visualization with TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be76b9a",
   "metadata": {},
   "source": [
    "We will now see how we can use TensorBoard to get more in-depth visualizations of our training process and our model. Let's start by cleaning up and making some additional imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32f7dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "np.random.seed(231)\n",
    "tf.random.set_seed(631)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ccc522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a643f9f",
   "metadata": {},
   "source": [
    "We can now load TensorBoard into the Jupyter Notebook and specify where the relevant logs should be kept that are needed to visualize the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c194140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735e1874",
   "metadata": {},
   "source": [
    "We build the same baseline model as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac06e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    tf.keras.layers.Dense(10, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mean_squared_error\",\n",
    "              optimizer=tf.keras.optimizers.SGD(learning_rate=0.03))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af3b200",
   "metadata": {},
   "source": [
    "We train our model as before, but we add a so-called `callback`. There are many types of callbacks, and you will discover some of them later, but TensorBoard brings its very own one.\n",
    "\n",
    "Note that we are defining a list of `callbacks`, but for now, we keep it at a single one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6596355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(logdir)\n",
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62ed3a2",
   "metadata": {},
   "source": [
    "In the training process, nothing has chaged, except that you will now find a log in your working directory. We can call upon that log to display the TensorBoard as follows (if you get an error, keep reading below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447420cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58461dac",
   "metadata": {},
   "source": [
    "You may run into an issue here because of a conflict between TensorBoard and the environment you are using. If this is the case, you need to specify exactly where TensorBoard can be found. It should be within your environment's folder, under \"bin\". You can find an example below. Uncomment the next lines and adjust your path accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2eabd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['TENSORBOARD_BINARY'] = '/Users/philippe/anaconda3/envs/adl_env/bin/tensorboard'\n",
    "# %tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430f03a1",
   "metadata": {},
   "source": [
    "Enjoy exploring the TensorBoard. We will see later that we can still add a lot more functionality to it. It is particularly nice for comparing different models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cb27bd",
   "metadata": {},
   "source": [
    "## 4.3 Saving and loading models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a2294d",
   "metadata": {},
   "source": [
    "Sometimes, we need to save our models, instead of training them from scratch (training deep neural networks can take quite some time!) TensorFlow enables this quite easily, for example with the following code. Once you have saved the model, check your working directory and you will find it there (as an \"h5\" file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740ced5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_tf_regression_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9848128e",
   "metadata": {},
   "source": [
    "We can now call up the (trained) model, just as easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3923909b",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = tf.keras.models.load_model(\"my_tf_regression_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dcc521",
   "metadata": {},
   "source": [
    "Verify that this is the trained model by running the predictions on it. You should get the same result as with the original model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8479c123",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_test[:5]\n",
    "y_predict = saved_model.predict(X_new)\n",
    "y_predict = y_predict.reshape(-1,)\n",
    "y_predict = [round(y,3) for y in y_predict]\n",
    "print(\"Predicted values: \" + str(y_predict))\n",
    "print(\"True values: \" + str(y_test[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb281e75",
   "metadata": {},
   "source": [
    "Sometimes, you might want to save only the parameters of the model, not the entire model. Again, TensorFlow allows us to save and load the parameters quite easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1108b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"my_tf_regression_weights.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e09f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"my_tf_regression_weights.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ab8525",
   "metadata": {},
   "source": [
    "## 4.4 Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e212993f",
   "metadata": {},
   "source": [
    "When using TensorBoard, we already saw a Callback in action. We will now get back to those, more generally. The idea is, that you might want to \"do stuff\" while training your model (for example, saving your current progress, in case your computer crashes). We start by defining our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c319f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "np.random.seed(231)\n",
    "tf.random.set_seed(631)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fbba82",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    tf.keras.layers.Dense(10, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mean_squared_error\",\n",
    "              optimizer=tf.keras.optimizers.SGD(learning_rate=0.03))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8935efa0",
   "metadata": {},
   "source": [
    "Let's now create a `ModelCheckpoint`, which is a call-back that saves your model in each iteration (epoch) of training. That is, whatever you might do later on with your model, you can always get back to the trained version.\n",
    "\n",
    "In your checkpoint, you will need to specify how the model should be saved as. Moreover, if you are using `validation_data` when training your model, you can also specify `save_best_only=True`. This means, that the callback looks at your validation_loss, and only saves the model at its current state if the validation loss is better (i.e. lower) than the one in the previous saved version. Quite useful, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443be9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"my_best_tf_regression_model.h5\", save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705d9c12",
   "metadata": {},
   "source": [
    "Once we have generated the callback, we add it to our callback list in training, as we did for TensorBoard. Nothing else changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244eab26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09398045",
   "metadata": {},
   "source": [
    "You can now run `tf.keras.models.load_model(\"my_best_tf_regression_model.h5\")` to call up the version of your model that had the best validation loss.\n",
    "\n",
    "Another useful callback to try out (this is left to you) is the `EarlyStopping` callback, which stops the training process if the validation loss isn't improving anymore. You can call it as follows:\n",
    "```\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "```\n",
    "\n",
    "Here, `patience` defines the number of epochs which the callback waits for improvements before stopping the training. `restore_best_weights=True` means that, once training is stopped, you get the version of the model that led to the best validation loss, not the final version of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5e7599",
   "metadata": {},
   "source": [
    "# 5. Multi-class classification with softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f181f1",
   "metadata": {},
   "source": [
    "We will now see how to use a softmax activation function to classify observations into multiple classes. As always, we start by cleaning up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11db922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "np.random.seed(583)\n",
    "tf.random.set_seed(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee060f30",
   "metadata": {},
   "source": [
    "For this example, we will use the full digits dataset (recall that we had images of ten sign-language digits, with the corresponding one-hot-encoded labels). We will split the dataset into training, test and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831efd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('digits_X.npy')\n",
    "y = np.load('digits_y.npy')\n",
    "X_train, X_other, y_train, y_other = train_test_split(X, y, train_size=0.8, random_state=342)\n",
    "X_test, X_valid, y_test, y_valid = train_test_split(X_other, y_other, train_size = 0.5, random_state=152)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a573c8",
   "metadata": {},
   "source": [
    "We train a simple feed-forward network, with two hidden layers, plus the softmax layer. Remember also that we need to flatten the image-matrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f7b545",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(64, 64)),\n",
    "    tf.keras.layers.Dense(300, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab01c851",
   "metadata": {},
   "source": [
    "As always, it is useful to take a look at our model and try to spot any possible issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6beaf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d487f12",
   "metadata": {},
   "source": [
    "We can then compile the model. For multiple categories, we will usually use the `categorical_crossentropy` loss (with some exceptions, one of which you will come across in this week's homework). We also use the `'adam'` optimization algorithm (with our self-defined learning rate), which we talk about in class next week. Otherwise, let's again keep track of `'accuracy'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe6d690",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682f9215",
   "metadata": {},
   "source": [
    "We can now run the model, we will use 200 epochs for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ee802f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = model.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e82061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(log.history['accuracy'],label = \"training accuracy\")\n",
    "plt.plot(log.history['val_accuracy'], label = \"validation accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e2e6bf",
   "metadata": {},
   "source": [
    "It seems we are experiencing some over-fitting. This is an issue that we will turn to next week!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e61c71",
   "metadata": {},
   "source": [
    "Finally, let's evaluate the model on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62b86d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31838b2",
   "metadata": {},
   "source": [
    "# 6. More multiclass-classification (homework)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c824b11",
   "metadata": {},
   "source": [
    "For this part, we will train a neural network on a dataset of fashion-products that is labeled with the categories of each product. The data is loaded directly from TensorFlow (which has quite the broad collection of datasets):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce81c1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(X_other, y_other), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7231a214",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_other.shape)\n",
    "print(y_other.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc37aff5",
   "metadata": {},
   "source": [
    "The X's are matrices (with 28x28 pixels), while the y's are numbers.\n",
    "\n",
    "We divide the values of X by 255 (essentially standardizing the pixel-values to 0-1) and also split apart a validation set (of the same size as the test set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09935b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_other = X_other / 255.\n",
    "X_test = X_test / 255.\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_other, y_other, train_size = 50000, random_state=152)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d502fd8f",
   "metadata": {},
   "source": [
    "Let's plot two examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1199ffe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(X_train[0],cmap=\"binary\")\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(X_train[500],cmap=\"binary\")\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce05e70f",
   "metadata": {},
   "source": [
    "As well as the corresponding labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89478fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train[0])\n",
    "print(y_train[500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4802b77",
   "metadata": {},
   "source": [
    "That's a bit hard to interpret. Luckily, we have the right names for each label available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cfed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf13cd27",
   "metadata": {},
   "source": [
    "We can now take another look at what the pictures above represent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce69c4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_names[y_train[0]])\n",
    "print(class_names[y_train[500]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473138d8",
   "metadata": {},
   "source": [
    "## 6.1 Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83032bb9",
   "metadata": {},
   "source": [
    "Let's start with some housekeeping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeff742",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "np.random.seed(451)\n",
    "tf.random.set_seed(346)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2ebe4f",
   "metadata": {},
   "source": [
    "Can you create a model with two hidden layers and one (softmax) output layer? The hidden layers should have 100 neurons each. You will have to figure out the number of neurons on the output layer (hint: it depends on the number of classes).\n",
    "\n",
    "Also, don't forget to flatten the images with an appropriate `input_shape`!\n",
    "\n",
    "Make sure you save your model as `model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaabf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455e72fb",
   "metadata": {},
   "source": [
    "Use the `summary` function, to see whether everything worked out as it should. If you defined the model as discussed above, you should get a total of 89,610 parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25373f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f91d79b",
   "metadata": {},
   "source": [
    "We can now compile the model. Use `optimizer=tf.keras.optimizers.SGD(learning_rate=0.01)` and `metrics=['accuracy']`. For the loss, use `sparse_categorical_crossentropy`. This is because our y's here are **not** one-hot-encoded, but instead are values from 0 to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d71356",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a9e0f4",
   "metadata": {},
   "source": [
    "Train the model for 30 epochs, keeping track also of the `validation_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee79f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e175357c",
   "metadata": {},
   "source": [
    "Take a look at the training process. The below will only work if you have called your training output `log`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d30a452",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(log.history['accuracy'],label = \"training accuracy\")\n",
    "plt.plot(log.history['val_accuracy'], label = \"testing accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d38f9b",
   "metadata": {},
   "source": [
    "If we accept our model, we can evaluate it on the test set, using the `evaluate` function of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c8dace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "761f747a",
   "metadata": {},
   "source": [
    "We can also take a look at some predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6df529",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_test[:4]\n",
    "y_predict = np.argmax(model.predict(X_new), axis=-1)\n",
    "y_predict = [class_names[y] for y in y_predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4305585",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(X_test[0],cmap=\"binary\")\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(X_test[1],cmap=\"binary\")\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(X_test[2],cmap=\"binary\")\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(X_test[3],cmap=\"binary\")\n",
    "plt.axis('off')\n",
    "print(\"Predictions are: \" + str(y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692bda86",
   "metadata": {},
   "source": [
    "Can you do better? Try tweaking the learning rate, the number of layers, and the neurons per layer to see if your validation loss improves. Once you have decided on your final model, evaluate it on the test set and note down your loss there."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adl_env",
   "language": "python",
   "name": "adl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
