{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d43d73f0",
   "metadata": {},
   "source": [
    "# Programming a deep feed-forward network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d5f1fc",
   "metadata": {},
   "source": [
    "This notebook is based on a fabulous [Kaggle tutorial by DATAI](https://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners) and uses the \"sign language digits data set\", also found through the link.\n",
    "\n",
    "We start by loading the relevant packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d5101f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb4243e",
   "metadata": {},
   "source": [
    "## 1. The dataset (this part is identical to the logistic regression exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9f83c1",
   "metadata": {},
   "source": [
    "The dataset contains 64x64 images of the signs used to represent the ten digits, 0-9. Indexes 204 to 408 of the dataset show the sign for zero and indexes 822 to 1027 show the sign for one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb919a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('digits_X.npy')\n",
    "y = np.load('digits_y.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49065d90",
   "metadata": {},
   "source": [
    "Each value of `X` is a matrix with pixel values, while each value of `y` is a vector representing the value of the digit (one-hot encoded):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32a988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[204].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a21b785",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[204]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599c006c",
   "metadata": {},
   "source": [
    "We can, of course, display the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da45e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 64\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(X[204].reshape(img_size, img_size))\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(X[822].reshape(img_size, img_size))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc147df",
   "metadata": {},
   "source": [
    "We only need the zeros and ones for our purposes. Hence, start by gathering only the relevant X-variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b43afbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((X[204:409], X[822:1028] ), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55819fbd",
   "metadata": {},
   "source": [
    "For the ys, we also only want the relevant ones. Moreover, we want to make sure that instead of a vector, we simply have 0 if the digit is zero and 1 if it is one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c2ad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.zeros(409-204)\n",
    "o = np.ones(1028-822)\n",
    "y = np.concatenate((z, o), axis=0).reshape(X.shape[0],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7298d019",
   "metadata": {},
   "source": [
    "With the `reshape`, we make sure that `y` is a vector with two dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60642dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca75ea4",
   "metadata": {},
   "source": [
    "Next, we split the data into training and testing with 15% in the test set (you know the drill):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf7efc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=172)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d622c30e",
   "metadata": {},
   "source": [
    "Finally, we need to \"flatten\" the Xs. Currently, our input is three-dimensional (each observation is a matrix). However, when we run regressions (or train models more generally), we usually have two-dimensional inputs, as it makes things a lot easier to work with. There are exceptions to this of course, specifically when using convolutional neural networks, but let's not get ahead of ourselves.\n",
    "\n",
    "What we will do is to convert each matrix (each observation's X-value) to a vector, simply by stacking all the columns of the matrix. If $X^{(i)} \\in \\mathcal{R}^{n \\times m}$, then the fitting vector $\\hat{X}^{(i)} \\in \\mathcal{R}^{n m}$. So we reshape accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123cb6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flat = X_train.reshape(X_train.shape[0],X_train.shape[1]*X_train.shape[2])\n",
    "X_test_flat = X_test.reshape(X_test.shape[0],X_test.shape[1]*X_test.shape[2])\n",
    "print(X_train_flat.shape)\n",
    "print(X_train_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c725fb6",
   "metadata": {},
   "source": [
    "We have 4096 pixels per observation, neatly stacked in a vector. All observations together (349 for train, 62 for test), gives us a (two-dimensional) matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3499d6fa",
   "metadata": {},
   "source": [
    "## 2. A neural network with an (arbitrarily large) hidden layer of neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f547f6",
   "metadata": {},
   "source": [
    "This time, we are creating a model that uses multiple neurons instead of just one. In particular, we will use one hidden layer with `hidden_layer_size` neurons (and the ReLU activation function), and an output layer with a single neuron performing the final binary classification (what activation function do we use here?)\n",
    "\n",
    "The principle approach is the same as before:\n",
    "\n",
    "0. Choose hyperparameters\n",
    "1. Initialize the model parameters $\\theta$ (random weights!)\n",
    "2. Until we cannot improve the cost function anymore (or we reach a certain numer of iterations):\n",
    "- Given your current model parameters, compute the cost function $J(\\theta)$ (forward propagation)\n",
    "- From the cost function, go backward to compute all the relevant derivatives (back-propagation)\n",
    "- Update the parameters: $\\theta := \\theta - \\alpha \\nabla_{\\theta} J(\\theta)$\n",
    "\n",
    "### Initialization\n",
    "\n",
    "Keep in mind that each neuron in the hidden layer has weights for all $m$ incoming edges (i.e. one for each `dimension`), as well as one bias term. The single neuron in the output layer has one weight for each of its incoming edges, as well as its own bias term.\n",
    "\n",
    "We will usually use dictionaries to store parameters when we have many. The function below is partially completed for you - can you finish it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa5972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(seed=392,dimension=4096,hidden_layer_size=3):\n",
    "    np.random.seed(seed)\n",
    "    parameters = {'weights1': np.random.rand(dimension,hidden_layer_size)*0.01,  # Use np.random.rand(dim1,dim2)*0.01, inputing the correct dimensions\n",
    "                  'bias1': np.zeros((1,hidden_layer_size)),   # Use np.zeros(shape), inputing the correct shape\n",
    "                  'weights2': np.random.randn(hidden_layer_size,1)*0.01,   # Use np.random.randn(dim1,dim2)*0.01, inputing the correct dimensions\n",
    "                  'bias2': np.zeros((1,1))}    # Use np.zeros(shape), inputing the correct shape\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d6a5c8",
   "metadata": {},
   "source": [
    "A quick try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8077456",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters(seed=123,dimension=4096,hidden_layer_size=3)\n",
    "print(parameters[\"weights1\"])\n",
    "print(parameters[\"bias1\"])\n",
    "print(parameters[\"weights2\"])\n",
    "print(parameters[\"bias2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89236afc",
   "metadata": {},
   "source": [
    "### Forward propagation\n",
    "\n",
    "The forward propagation step is quite similar to what we saw in the logistic regression. Of course, our model for $\\hat{y}$ is now a whole lot more complex. But that doesn't matter: the neural network is essentially a computation graph, so we just go layer by layer, and computations are quite easy at each layer. We will make use of two helper functions to compute the ReLU activation (at the hidden layer) and the logistic sigmoid activation (at the output layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df91205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0,z)     # for proper vectorization, you might want to look up `np.maximum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d234fb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(z):\n",
    "    return 1/(1 + np.exp(-z))  # you have seen this before in logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56e1684",
   "metadata": {},
   "source": [
    "Remember that we want to make fast computations. Hence, our functions need to be able to take a whole matrix of values and compute the activation for each element (they need to be \"vectorized\"). Try it out for both activation functions below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0c9806",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.array([[1,-2,3],\n",
    "              [0,6,-2],\n",
    "              [3,-1,0]])\n",
    "print(relu(Z))\n",
    "print(sigma(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa8c612",
   "metadata": {},
   "source": [
    "Next comes the actual forward propagation step. Remember that we need to compute:\n",
    "1. For each neuron at the first layer:\n",
    "- $Z^{[1]}$ = the weighted sum of the inputs X, to which we add the bias\n",
    "- $A^{[1]}$ = the actual activation: the neuron's activation function applied to $Z^{[1]}$\n",
    "2. For the neuron at the second layer:\n",
    "- $Z^{[2]}$ = the weighted sum of the inputs $A^{[1]}$, to which we add the bias\n",
    "- $A^{[2]}$ = the actual activation: the neuron's activation function applied to $Z^{[2]}$\n",
    "3. The cost function, given $\\hat{y} = A^{[2]}$: We will stick with what we saw before in binary classification, so $J=\\frac{1}{n}\\sum_{i=1}^n L^{(i)}$ with $L^{(i)} = -y^{(i)} \\log \\hat y^{(i)} - (1-y^{(i)}) (1-\\log \\hat y^{(i)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda3efc7",
   "metadata": {},
   "source": [
    "Aside from the cost, the forward propagation should return the computed Z's and A's in what we call a \"cache\". This is important for back-propagation later down the line.\n",
    "\n",
    "We start with a naive implementation, where we compute the activations for each neuron separately. Do you see what happens in the function below? Can you complete the missing pieces?\n",
    "\n",
    "A few hints:\n",
    "- you might benefit from adding print statements and trying out the function using `forward_propagation_naive(X_train_flat,y_train,parameters,2)`\n",
    "- The input matrix $X$ has dimensions $(n,m)$, where $n$ is the number of observations and $m$ the number of features\n",
    "- The weight matrix $W^{[1]}$ has dimensions $(m,\\text{hidden_layer_size})$\n",
    "- The bias vector $b^{[1]}$ has dimensions $(1,\\text{hidden_layer_size})$\n",
    "- The weight matrix $W^{[2]}$ has dimensions $(\\text{hidden_layer_size},1)$\n",
    "- The bias vector $b^{[2]}$ has dimensions $(1,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a273ead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_naive(X,y,parameters,hidden_layer_size):\n",
    "    # Layer 1 (hidden layer)\n",
    "    Z1 = np.zeros((X.shape[0],hidden_layer_size))\n",
    "    A1 = np.zeros((X.shape[0],hidden_layer_size))\n",
    "    for neuron in range(hidden_layer_size):\n",
    "        w = parameters['weights1'][:,neuron] # find the right weight (recall that we stacked our weights with shape (in,out))\n",
    "        b = parameters['bias1'][0,neuron]    # find the right bias term (recall that we stacked our biases with shape (1,out))\n",
    "        z = np.dot(X,w) + b                  # compute z, using np.dot. Think of the correct dimensions!\n",
    "        Z1[:,neuron] = z\n",
    "        A1[:,neuron] = relu(z)\n",
    "    \n",
    "    # Layer 2 (output layer)\n",
    "    Z2 = np.dot(A1,parameters['weights2']) + parameters['bias2'] # at the second layer, there is only one node. Use np.dot again, and watch out for the correct dimensions\n",
    "    A2 = sigma(Z2)\n",
    "    \n",
    "    # Compute the cost\n",
    "    yHat = A2\n",
    "    cost = np.sum(-y*np.log(yHat) - (1-y)*np.log(1-yHat))/X.shape[0]\n",
    "    \n",
    "    # Compute the cache\n",
    "    cache = {'Z1': Z1,\n",
    "             'A1': A1,\n",
    "             'Z2': Z2,\n",
    "             'A2': A2}\n",
    "    \n",
    "    return cost, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c24b462",
   "metadata": {},
   "source": [
    "Try it out. If there are no mistake, the code below should print out 0.7387257645994343"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f49fd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters(seed=123,dimension=4096,hidden_layer_size=3)\n",
    "cost, _ = forward_propagation_naive(X_train_flat,y_train,parameters,3)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb48a7e9",
   "metadata": {},
   "source": [
    "We know that vectorization is faster, so we will vectorize not just on the observations, but also on the neurons within a layer (and then see that this is quite a bit faster). Can you complete the function?\n",
    "\n",
    "A few additional hints here:\n",
    "- Each neuron has its own total input z for each observations. Hence, $Z^{[1]}$ should have dimensions $(n,\\text{hidden_layer_size})$\n",
    "- The same logic holds for the neuron at the second layer. Hence, $Z^{[2]}$ should have dimensions $(n,1)$\n",
    "- The activation matrix $A^{[l]}$ has the same dimensions as the total input matrix $Z^{[l]}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6bc8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X,y,parameters):\n",
    "    # Layer 1 (hidden layer)\n",
    "    Z1 = np.dot(X,parameters['weights1']) + parameters['bias1']  # Use np.dot!\n",
    "    A1 = relu(Z1)\n",
    "    \n",
    "    # Layer 2 (output layer)\n",
    "    Z2 = np.dot(A1,parameters['weights2']) + parameters['bias2'] # Use np.dot!\n",
    "    A2 = sigma(Z2)\n",
    "    \n",
    "    # Compute the cost - this is exactly as before!\n",
    "    yHat = A2\n",
    "    cost = np.sum(-y*np.log(yHat) - (1-y)*np.log(1-yHat))/X.shape[0]\n",
    "    \n",
    "    # Compute the cache\n",
    "    cache = {'Z1': Z1,\n",
    "             'A1': A1,\n",
    "             'Z2': Z2,\n",
    "             'A2': A2}\n",
    "    \n",
    "    return cost, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bb57dd",
   "metadata": {},
   "source": [
    "Try it out. If there are no mistake, the code below should print out\n",
    "1. 0.7387257645994343\n",
    "1. (349,3)\n",
    "1. (349,3)\n",
    "1. (349,1)\n",
    "1. (349,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b180cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters(seed=123,dimension=4096,hidden_layer_size=3)\n",
    "cost, cache = forward_propagation(X_train_flat,y_train,parameters)\n",
    "print(cost)\n",
    "print(cache['Z1'].shape)\n",
    "print(cache['A1'].shape)\n",
    "print(cache['Z2'].shape)\n",
    "print(cache['A2'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80773e10",
   "metadata": {},
   "source": [
    "Let's now compare the difference in computation time. What we will do is to create 200 sets of initial parameters for a network of width 10 and apply forward propagation once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1538f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 200\n",
    "time_naive = 0\n",
    "cost_naive = 0\n",
    "time_vectorized = 0\n",
    "cost_vectorized = 0\n",
    "\n",
    "for it in range(iterations):\n",
    "    parameters = initialize_parameters(seed=np.random.randint(1),dimension=4096,hidden_layer_size=10)\n",
    "    # Running things with a for-loop:\n",
    "    tic = time.process_time()\n",
    "    cost,_ = forward_propagation_naive(X_train_flat,y_train,parameters,10)\n",
    "    toc = time.process_time()\n",
    "    time_naive += 1000*(toc-tic)\n",
    "    cost_naive += cost\n",
    "    # Running things \"vectorized\":\n",
    "    tic = time.process_time()\n",
    "    cost,_ = forward_propagation(X_train_flat,y_train,parameters)\n",
    "    toc = time.process_time()\n",
    "    time_vectorized += 1000*(toc-tic)\n",
    "    cost_vectorized += cost\n",
    "\n",
    "print (\"Naive: Cost = \" + str(cost_naive/iterations) + \", computation time = \" + str(time_naive/iterations) + \"ms\")\n",
    "print (\"Vectorized: Cost = \" + str(cost_vectorized/iterations) + \", computation time = \" + str(time_vectorized/iterations) + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8f810c",
   "metadata": {},
   "source": [
    "### Back-propagation\n",
    "\n",
    "We move onto the second step of our update: finding the gradients. Make sure you use the chain rule. We will discuss in the tutorial how to derive the derivatives, but for the programming part, the relevant computations can be found below:\n",
    "- `dZ2` $= \\nabla_{Z^{[2]}} J = \\frac{1}{n}(A^{[2]} - y)$  (this should give you a $(n,1)$ matrix - why?)\n",
    "- `dW2` $=\\nabla_{W^{[2]}} J  = (A^{[1]})^T  (\\nabla_{Z^{[2]}} J)$ (this should give you a $(\\text{hidden_layer_size},1)$ matrix - why?)\n",
    "- `db2` $=\\nabla_{b^{[2]}} J = \\sum_{i=1}^n \\frac{\\partial J}{\\partial z_1^{[2](i)}}$ (you are summing up over the entries of dZ2)\n",
    "- `dZ1` $= \\nabla_{Z^{[1]}} J = (\\nabla_{Z^{[2]}} J) (W^{[2]})^T \\circ E^{[1]}$. Here, $\\circ$ is element-wise multiplication and $E^{[1]}$ is a matrix of the same dimensions as $Z^{[1]}$ that is 1 when the entry is positive and 0 otherwise (this should give you a $(n,\\text{hidden_layer_size})$ matrix - why?)\n",
    "- `dW1` $=\\nabla_{W^{[1]}} J  = (X^T)(\\nabla_{Z^{[1]}} J)$ (this should give you a $(m,\\text{hidden_layer_size})$ matrix - why?)\n",
    "- `db1` = $\\left[ \\nabla_{b^{[1]}_1} J, \\nabla_{b^{[1]}_2} J,..., \\nabla_{b^{[1]}_{\\text{hidden_layer_size}}} J \\right] = \\left[\\sum_{i=1}^n \\frac{\\partial J}{\\partial z_1^{[1](i)}}, \\sum_{i=1}^n \\frac{\\partial J}{\\partial z_2^{[1](i)}}, ..., \\sum_{i=1}^n \\frac{\\partial J}{\\partial z_{\\text{hidden_layer_size}}^{[1](i)}} \\right]$ (you are summing up over **one** of the axes of dZ1 - be careful to choose the right one)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2b9a78",
   "metadata": {},
   "source": [
    "A final hint about computing $E^{[1]}$. See the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d83b4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.array([[1,-2,3],\n",
    "              [0,6,-2],\n",
    "              [3,-1,0]])\n",
    "np.where(Z>0,1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76da48a9",
   "metadata": {},
   "source": [
    "We can now define the back-propagation step, which returns the gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece447e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(X,y,parameters,cache):\n",
    "    dZ2 = (cache['A2'] - y)/X.shape[0]\n",
    "    dW2 = np.dot(cache['A1'].T,dZ2)\n",
    "    db2 = np.sum(dZ2,axis=0,keepdims=True)\n",
    "    dZ1 = np.dot(dZ2,parameters['weights2'].T)*np.where(cache['Z1']>0,1,0)\n",
    "    dW1 = np.dot(X.T,dZ1)\n",
    "    db1 = np.sum(dZ1,axis=0,keepdims=True) # make sure to sum up over the right axis (see the np.sum documentation), and to set keepdims=True\n",
    "    grads = {'weights1': dW1,\n",
    "             'bias1': db1,\n",
    "             'weights2': dW2,\n",
    "             'bias2': db2}\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23bfa61",
   "metadata": {},
   "source": [
    "If everything is programmed correctly, the below code should print out\n",
    "\n",
    "1. -0.000508086877680383\n",
    "1. (4096, 3)\n",
    "1. (1, 3)\n",
    "1. (3, 1)\n",
    "1. (1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d982a7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters(seed=123,dimension=4096,hidden_layer_size=3)\n",
    "cost, cache = forward_propagation(X_train_flat,y_train,parameters)\n",
    "grads = back_propagation(X_train_flat,y_train,parameters,cache)\n",
    "print(grads['weights1'][0,0])\n",
    "print(grads['weights1'].shape)\n",
    "print(grads['bias1'].shape)\n",
    "print(grads['weights2'].shape)\n",
    "print(grads['bias2'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5817938b",
   "metadata": {},
   "source": [
    "### Putting it together: parameter updating and training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4df55ce",
   "metadata": {},
   "source": [
    "In each iteration of our learning procedure, we update the parameters, hopefully moving closer towards the optimum. How we update the parameters is determined by the gradient, as well as the learning rate (a hyper-parameter). Let's define one update step:\n",
    "1. Compute the `forward_propagation` step (returning `cost` and `cache`)\n",
    "1. Compute the `back_propagation` step (using `cache` from `forward_propagation`)\n",
    "1. Update each entry in `parameters` as follows: $\\theta := \\theta - \\alpha \\nabla_{\\theta} J$ (Because we made sure above that the shapes are \"in the right way\", we don't have to worry about individual parameters, but can update whole groups - also, we made sure that the parameters and their gradients are referenced in the same way in both dictionaries. Note that $\\alpha$ is the learning rate)\n",
    "1. Return the updated dictionary `parameters` and the `cost` from `forward_propagation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a82d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_update(X,y,parameters,learning_rate):\n",
    "    cost, cache = forward_propagation(X,y,parameters)\n",
    "    grads = back_propagation(X,y,parameters,cache)\n",
    "    for entry in parameters:\n",
    "        parameters[entry] = parameters[entry] - learning_rate*grads[entry]#\n",
    "    return parameters, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5141b4",
   "metadata": {},
   "source": [
    "We can now train our model, by running the parameter update multiple times. We will use 3 neurons at the hidden layer, a learning rate of 0.01 and run the algorithm for 2,500 iterations. Can you adjust the function below? Make sure to initialize the parameters with our custom-made function. Also, each time you run the parameter-update, store the resulting `cost` in a list `cost_list`. At the end, return the final `parameter` dictionary and the `cost_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc2cf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(X,y,hidden_layer_size=3,learning_rate=0.01,iterations=2500,verbose=True):\n",
    "    parameters = initialize_parameters(seed=np.random.randint(1),dimension=X.shape[1],hidden_layer_size=hidden_layer_size) # Initialize the parameters\n",
    "    cost_list = []\n",
    "    for it in range(iterations):\n",
    "        parameters,cost = parameter_update(X,y,parameters,learning_rate) # for each iteration, update the parameters using forward and back propagation\n",
    "        cost_list.append(cost)  # Also, make sure to add the cost to the cost_list\n",
    "        if verbose:\n",
    "            print('Cost after iteration %i: %f' %(it,cost))\n",
    "    return parameters, cost_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37873d5d",
   "metadata": {},
   "source": [
    "Now, train the model and display the training loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fdda19",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, cost_list = model_training(X_train_flat,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0e3cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(cost_list)),cost_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b362b9",
   "metadata": {},
   "source": [
    "### Making predictions\n",
    "\n",
    "We don't just want to train a neural network, we also want to use it to make predictions. For this purpose, we create a `predict` function, that takes an input X, as well as the parameters of the trained model.\n",
    "\n",
    "Don't worry about computing the prediction - we have already done so, when we implemented the forward propagation. Note that forward propagation takes as input both an `X` and a `y`, but we don't care about the cost (only about the `yHat = cache['A2']`, so we can give an empty `y`, as long as it has the correct shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b97116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,parameters):\n",
    "    _, cache = forward_propagation(X,np.zeros((X.shape[0],1)),parameters)\n",
    "    yHat = cache['A2']    # Get yHat from the cache\n",
    "    y_prediction = (yHat > 0.5)      # Make a prediction - when yHat > 0.5, assume 1, otherwise 0\n",
    "    return y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702bc17c",
   "metadata": {},
   "source": [
    "Let's see how well our predictions perform, both on the training set and the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd72c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction_test = predict(X_test_flat,parameters)\n",
    "y_prediction_train = predict(X_train_flat,parameters)\n",
    "\n",
    "print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n",
    "print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b778caba",
   "metadata": {},
   "source": [
    "Getting there! Certainly some overfitting happening, but 95% test accuracy is not bad at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97126168",
   "metadata": {},
   "source": [
    "## 3. A neural network with arbitrarily many, arbitrarily large hidden layers of ReLUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e8555b",
   "metadata": {},
   "source": [
    "Let's go deeper. We are creating a model that uses multiple hidden layers. In particular, the `L-1` hidden layers will have sizes `hidden_layer_sizes` = $[n_1,n_2,...,n_{L-1}]$ (and the ReLU activation function). In addition, there is again an output layer with a single neuron performing the final binary classifcation (of course, using the logistic sigmoid function).\n",
    "\n",
    "We use exactly the same approach as before, just that we need to \"automate\" our computations a bit more.\n",
    "\n",
    "### Initialization\n",
    "\n",
    "Keep in mind that each neuron in the hidden layer $l$ has weights for all $n_{l-1}$ incoming edges, as well as one bias term (in the case of layer 1, $n_0$ are the number of incoming edges from the input, so the number of features of `X`).\n",
    "\n",
    "We will usually use dictionaries to store parameters when we have many. Note the naming convention for layers starting from 1 doesn't really fit well with the typical naming convention of Python. But we can make our life easier by treating the input layer as \"Layer 0\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83e82fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(seed=392,dimension=4096,hidden_layer_sizes=[3,3,3]):\n",
    "    np.random.seed(seed)\n",
    "    parameters = {}\n",
    "    hidden_layer_sizes = [dimension] + hidden_layer_sizes\n",
    "    \n",
    "    for l in range(1,len(hidden_layer_sizes)+1):\n",
    "        size_in = hidden_layer_sizes[l-1]\n",
    "        if l < len(hidden_layer_sizes):\n",
    "            size_out = hidden_layer_sizes[l]\n",
    "            parameters['weights' + str(l)] = np.random.rand(size_in,size_out)*0.1 - 0.05    # Note the different initialization of parameters. This is to help learning along\n",
    "        else:\n",
    "            size_out = 1\n",
    "            parameters['weights' + str(l)] = np.random.rand(size_in,size_out)*0.1 - 0.05    # Note the different initialization of parameters. This is to help learning along\n",
    "        parameters['bias' + str(l)] = np.zeros((1,size_out))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccf1c84",
   "metadata": {},
   "source": [
    "A quick try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a68fe0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters(seed=123,dimension=4096,hidden_layer_sizes=[3,3,3])\n",
    "print(parameters[\"weights1\"])\n",
    "print(parameters[\"bias1\"])\n",
    "print(parameters[\"weights2\"])\n",
    "print(parameters[\"bias2\"])\n",
    "print(parameters[\"weights3\"])\n",
    "print(parameters[\"bias3\"])\n",
    "print(parameters[\"weights4\"])\n",
    "print(parameters[\"bias4\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750f23ca",
   "metadata": {},
   "source": [
    "### Forward propagation\n",
    "\n",
    "Forward propagation works the same as before, just over a few more layers. We will make use again of two helper functions to compute the ReLU activation (at the hidden layers) and the logistic sigmoid activation (at the output layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75aa2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a145c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(z):\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cfeef0",
   "metadata": {},
   "source": [
    "Next comes the actual forward propagation step. Remember that we need to compute:\n",
    "1. For each neuron at the hidden layers $l=1,...,L-1$:\n",
    "- $Z^{[l]}$ = the weighted sum of the inputs A^{[l-1]}, to which we add the bias (in the case of $Z^{[1]}$, we simply have $A^{[0]} = X$)\n",
    "- $A^{[l]}$ = the actual activation: the neuron's activation function applied to $Z^{[l]}$\n",
    "2. For the neuron at the output layer $L$:\n",
    "- $Z^{[L]}$ = the weighted sum of the inputs $A^{[L-1]}$, to which we add the bias\n",
    "- $A^{[L]}$ = the actual activation: the neuron's activation function applied to $Z^{[L]}$\n",
    "3. The cost function, given $\\hat{y} = A^{[L]}$: We will stick with what we saw before in binary classification, so $J=\\frac{1}{n}\\sum_{i=1}^n L^{(i)}$ with $L^{(i)} = -y^{(i)} \\log \\hat y^{(i)} - (1-y^{(i)}) (1-\\log \\hat y^{(i)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5d7d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X,y,parameters,hidden_layer_sizes):\n",
    "    cache = {}\n",
    "    \n",
    "    # Hidden layers\n",
    "    current_A = X\n",
    "    for l in range(1,len(hidden_layer_sizes)+1):\n",
    "        current_Z = np.dot(current_A,parameters['weights'+str(l)]) + parameters['bias'+str(l)]\n",
    "        current_A = relu(current_Z)\n",
    "        cache['Z'+str(l)] = current_Z\n",
    "        cache['A'+str(l)] = current_A\n",
    "\n",
    "    # Output layer\n",
    "    L = len(hidden_layer_sizes) + 1\n",
    "    current_Z = np.dot(current_A,parameters['weights'+str(L)]) + parameters['bias'+str(L)]\n",
    "    current_A = sigma(current_Z)\n",
    "    cache['Z'+str(L)] = current_Z\n",
    "    cache['A'+str(L)] = current_A\n",
    "    \n",
    "    # Compute the cost\n",
    "    yHat = current_A \n",
    "    cost = np.sum(-y*np.log(yHat) - (1-y)*np.log(1-yHat))/X.shape[0] \n",
    "\n",
    "    return cost, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b34ec65",
   "metadata": {},
   "source": [
    "Note: we could technically read the layers out from the parameters, but we make things a bit simpler here and simply give the layers as an input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05efe131",
   "metadata": {},
   "source": [
    "Try it out. If there are no mistake, the code below should print out\n",
    "1. 0.6931471805599453\n",
    "1. (349,3)\n",
    "1. (349,3)\n",
    "1. (349, 2)\n",
    "1. (349, 2)\n",
    "1. (349,1)\n",
    "1. (349,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef8049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_sizes = [3,2]\n",
    "parameters = initialize_parameters(seed=123,dimension=4096,hidden_layer_sizes=hidden_layer_sizes)\n",
    "cost, cache = forward_propagation(X_train_flat,y_train,parameters,hidden_layer_sizes)\n",
    "print(cost)\n",
    "print(cache['Z1'].shape)\n",
    "print(cache['A1'].shape)\n",
    "print(cache['Z2'].shape)\n",
    "print(cache['A2'].shape)\n",
    "print(cache['Z3'].shape)\n",
    "print(cache['A3'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3d8ab1",
   "metadata": {},
   "source": [
    "### Back-propagation\n",
    "\n",
    "We move onto the second step of our update: finding the gradients. We do essentially the same as before, just that we keep going backward from layer $L-1$. The following generalizations of the derivatives may be helpful for the output layer (note - nothing changes here from before except the indexing):\n",
    "\n",
    "- `dZ(L)` $= \\nabla_{Z^{[L]}} J = \\frac{1}{n}(A^{[L]} - y)$  (this should give you a $(n,1)$ matrix - why?)\n",
    "- `dW(L)` $=\\nabla_{W^{[L]}} J  = (A^{[L-1]})^T  (\\nabla_{Z^{[L]}} J)$ (this should give you a $(n_{L-1},1)$ matrix - why?)\n",
    "- `db(L)` $=\\nabla_{b^{[L]}} J = \\sum_{i=1}^n \\frac{\\partial J}{\\partial z_1^{[L](i)}}$ (you are summing up over the entries of dZ(L))\n",
    "\n",
    "And for the hidden layers $l=1,...,L-1$ (this is identical to the single hidden layer earlier, just that we generalize the indexes):\n",
    "\n",
    "- `dZ(l)` $= \\nabla_{Z^{[l]}} J = (\\nabla_{Z^{[l+1]}} J) (W^{[l+1]})^T \\circ E^{[l]}$. Here, $\\circ$ is element-wise multiplication and $E^{[l]}$ is a matrix of the same dimensions as $Z^{[l]}$ that is 1 when the entry is positive and 0 otherwise (this should give you a $(n,n_l)$ matrix - why?)\n",
    "- `dW(l)` $=\\nabla_{W^{[l]}} J  = (A^{[l-1]})^T (\\nabla_{Z^{[l]}} J)$ (this should give you a $(n_{l-1},n_l)$ matrix - why? Note that $A^{[0]} = X$, with $n_0 = m$)\n",
    "- `db(l)` = $\\left[ \\nabla_{b^{[l]}_1} J, \\nabla_{b^{[l]}_2} J,..., \\nabla_{b^{[l]}_{n_l}} J \\right] = \\left[\\sum_{i=1}^n \\frac{\\partial J}{\\partial z_1^{[l](i)}}, \\sum_{i=1}^n \\frac{\\partial J}{\\partial z_2^{[l](i)}}, ..., \\sum_{i=1}^n \\frac{\\partial J}{\\partial z_{n_{l}}^{[l](i)}} \\right]$ (you are summing up over **one** of the axes of dZ1 - be careful to choose the right one)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24381ca3",
   "metadata": {},
   "source": [
    "We can now define the back-propagation step, which returns the gradients. It might be helpful, instead of creating new variables `dZ` for each step, to simply overwrite the existing ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6363e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(X,y,parameters,cache,hidden_layer_sizes):\n",
    "    grads = {}\n",
    "    \n",
    "    # Output layer\n",
    "    L = len(hidden_layer_sizes) + 1\n",
    "    dZ = (cache['A' + str(L)] - y)/X.shape[0]\n",
    "    prev_A = cache['A' + str(L-1)]\n",
    "    dW = np.dot(prev_A.T,dZ)\n",
    "    db = np.sum(dZ,axis=0,keepdims=True)\n",
    "    grads['weights' + str(L)] = dW\n",
    "    grads['bias' + str(L)] = db\n",
    "    \n",
    "    # Hidden layers\n",
    "    for l in range(L-1,0,-1):\n",
    "        dZ = np.dot(dZ,parameters['weights' + str(l+1)].T)*np.where(cache['Z' + str(l)]>0,1,0)\n",
    "        if l > 1:\n",
    "            prev_A = cache['A' + str(l-1)]\n",
    "        else:\n",
    "            prev_A = X\n",
    "        dW = np.dot(prev_A.T,dZ)\n",
    "        db = np.sum(dZ,axis=0,keepdims=True)\n",
    "        grads['weights' + str(l)] = dW\n",
    "        grads['bias' + str(l)] = db\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823f3810",
   "metadata": {},
   "source": [
    "If everything is programmed correctly, the below code should print out\n",
    "\n",
    "1. -0.0014326647564469955\n",
    "1. (4096, 3)\n",
    "1. (1, 3)\n",
    "1. (3, 2)\n",
    "1. (1, 2)\n",
    "1. (2, 1)\n",
    "1. (1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d1147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_sizes = [3,2]\n",
    "parameters = initialize_parameters(seed=456,dimension=4096,hidden_layer_sizes=hidden_layer_sizes)\n",
    "cost, cache = forward_propagation(X_train_flat,y_train,parameters,hidden_layer_sizes)\n",
    "grads = back_propagation(X_train_flat,y_train,parameters,cache,hidden_layer_sizes)\n",
    "print(grads['bias3'][0,0])\n",
    "print(grads['weights1'].shape)\n",
    "print(grads['bias1'].shape)\n",
    "print(grads['weights2'].shape)\n",
    "print(grads['bias2'].shape)\n",
    "print(grads['weights3'].shape)\n",
    "print(grads['bias3'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4b493d",
   "metadata": {},
   "source": [
    "### Putting it together: parameter updating and training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29165672",
   "metadata": {},
   "source": [
    "The parameter updating step is identical to before, only that we need to add the `hidden_layer_sizes` input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc941985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_update(X,y,parameters,learning_rate,hidden_layer_sizes):\n",
    "    cost, cache = forward_propagation(X,y,parameters,hidden_layer_sizes)\n",
    "    grads = back_propagation(X,y,parameters,cache,hidden_layer_sizes)\n",
    "    for entry in parameters:\n",
    "        parameters[entry] = parameters[entry] - learning_rate*grads[entry]\n",
    "    return parameters, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03908a8",
   "metadata": {},
   "source": [
    "We can now train our model, by running the parameter update multiple times. We will use 3 neurons at first hidden layer and 2 neurons at the second hidden layer. Notice the high number of iterations, that will be needed (but later on, we see how one can improve upon that).\n",
    "\n",
    "Can you adjust the function below? Make sure to initialize the parameters with our custom-made function. Also, each time you run the parameter-update, store the resulting `cost` in a list `cost_list`. At the end, return the final `parameter` dictionary and the `cost_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f168826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(X,y,hidden_layer_sizes=[3,2],learning_rate=0.005,iterations=20000,verbose=True):\n",
    "    parameters = initialize_parameters(seed=np.random.randint(1),dimension=X.shape[1],hidden_layer_sizes=hidden_layer_sizes)\n",
    "    cost_list = []\n",
    "    for it in range(iterations):\n",
    "        parameters,cost = parameter_update(X,y,parameters,learning_rate,hidden_layer_sizes)\n",
    "        cost_list.append(cost)\n",
    "        if verbose:\n",
    "            print('Cost after iteration %i: %f' %(it,cost))\n",
    "    return parameters, cost_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821518d4",
   "metadata": {},
   "source": [
    "Now, train the model and display the training loss. This might take a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feee7d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_sizes = [3,2]\n",
    "parameters, cost_list = model_training(X_train_flat,y_train,hidden_layer_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4392248",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(cost_list)),cost_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5faf739",
   "metadata": {},
   "source": [
    "We can see already by looking at the report from the model training, and even more clearly in the graph, that we need quite some time to get to any sensible state with the model. If you play around with the initialization, you will notice that it is hard to even find value for which the model trains at all. This complexity in optimization is inherent to deep neural networks. Luckily, there are advanced optimization algorithms that speed things up (and increase our chances of even getting to a reasonable training result). We will learn about those in the next lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d277b596",
   "metadata": {},
   "source": [
    "### Making predictions\n",
    "\n",
    "As before, we can now use our model to make predictions. The function is basically as before, just that we have to make sure to get the correct activation matrix from the cache (the one at the last layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89282e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,parameters,hidden_layer_sizes=[3,2]):\n",
    "    _, cache = forward_propagation(X,np.zeros((X.shape[0],1)),parameters,hidden_layer_sizes)\n",
    "    yHat = cache['A' + str(len(hidden_layer_sizes)+1)]    # Get yHat from the cache (the last layer's activation!)\n",
    "    y_prediction = (yHat > 0.5)      # Make a prediction - when yHat > 0.5, assume 1, otherwise 0\n",
    "    return y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637b0714",
   "metadata": {},
   "source": [
    "Let's see how well our predictions perform, both on the training set and the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47aa2b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction_test = predict(X_test_flat,parameters,hidden_layer_sizes)\n",
    "y_prediction_train = predict(X_train_flat,parameters,hidden_layer_sizes)\n",
    "\n",
    "print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n",
    "print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db2cc32",
   "metadata": {},
   "source": [
    "A lot of training effort, and we actually did worse. With some regularization, we can probably do better, but we'll leave that for later."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adl_env",
   "language": "python",
   "name": "adl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
