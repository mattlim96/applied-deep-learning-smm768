{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fd1bc41",
   "metadata": {},
   "source": [
    "# Implementing advanced procedures and algorithms in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bdc8d6",
   "metadata": {},
   "source": [
    "This notebook collects advanced procedures relevant to training more complex neural networks. It gives you the possibility to look up the procedures as needed and copy the relevant code. The notebook builds on a GitHub repository by [Aurélien Géron](https://github.com/ageron/handson-ml2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21cd366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31838b2",
   "metadata": {},
   "source": [
    "# 1. From the previous notebook: Multiclass-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c824b11",
   "metadata": {},
   "source": [
    "Throughout, we will train a neural network on a dataset of fashion-products that is labeled with the categories of each product. The data is loaded directly from TensorFlow (which has quite the broad collection of datasets):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce81c1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(X_other, y_other), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7231a214",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_other.shape)\n",
    "print(y_other.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc37aff5",
   "metadata": {},
   "source": [
    "The X's are matrices (with 28x28 pixels), while the y's are numbers.\n",
    "\n",
    "We divide the values of X by 255 (essentially standardizing the pixel-values to 0-1) and also split apart a validation set (of the same size as the test set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09935b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_other = X_other / 255.\n",
    "X_test = X_test / 255.\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_other, y_other, train_size = 50000, random_state=152)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d502fd8f",
   "metadata": {},
   "source": [
    "Let's plot two examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1199ffe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(X_train[0],cmap=\"binary\")\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(X_train[500],cmap=\"binary\")\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce05e70f",
   "metadata": {},
   "source": [
    "As well as the corresponding labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89478fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train[0])\n",
    "print(y_train[500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4802b77",
   "metadata": {},
   "source": [
    "That's a bit hard to interpret. Luckily, we have the right names for each label available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cfed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf13cd27",
   "metadata": {},
   "source": [
    "We can now take another look at what the pictures above represent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce69c4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_names[y_train[0]])\n",
    "print(class_names[y_train[500]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473138d8",
   "metadata": {},
   "source": [
    "## 1.1 Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2ebe4f",
   "metadata": {},
   "source": [
    "Can you create a model with two hidden layers and one (softmax) output layer? The hidden layers should have 100 neurons each. You will have to figure out the number of neurons on the output layer (hint: it depends on the number of classes).\n",
    "\n",
    "Also, don't forget to flatten the images with an appropriate `input_shape`!\n",
    "\n",
    "Make sure you save your model as `model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaabf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455e72fb",
   "metadata": {},
   "source": [
    "Use the `summary` function, to see whether everything worked out as it should. If you defined the model as discussed above, you should get a total of 89,610 parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25373f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f91d79b",
   "metadata": {},
   "source": [
    "We can now compile the model. Use `optimizer=tf.keras.optimizers.SGD(learning_rate=0.01)` and `metrics=['accuracy']`. For the loss, use `sparse_categorical_crossentropy`. This is because our y's here are **not** one-hot-encoded, but instead are values from 0 to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d71356",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a9e0f4",
   "metadata": {},
   "source": [
    "Train the model for 30 epochs, keeping track also of the `validation_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee79f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e175357c",
   "metadata": {},
   "source": [
    "Take a look at the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d30a452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plot(log):\n",
    "    plt.plot(log.history['accuracy'],label = \"training accuracy\",color='green')\n",
    "    plt.plot(log.history['loss'],label = \"training loss\",color='darkgreen')\n",
    "    plt.plot(log.history['val_accuracy'], label = \"validation accuracy\",color='grey')\n",
    "    plt.plot(log.history['val_loss'], label = \"validation loss\",color='darkblue')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "create_plot(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d38f9b",
   "metadata": {},
   "source": [
    "If we accept our model, we can evaluate it on the test set, using the `evaluate` function of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c8dace",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761f747a",
   "metadata": {},
   "source": [
    "We can also take a look at some predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6df529",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_test[:4]\n",
    "y_predict = np.argmax(model.predict(X_new), axis=-1)\n",
    "y_predict = [class_names[y] for y in y_predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4305585",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(X_test[0],cmap=\"binary\")\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(X_test[1],cmap=\"binary\")\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(X_test[2],cmap=\"binary\")\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(X_test[3],cmap=\"binary\")\n",
    "plt.axis('off')\n",
    "print(\"Predictions are: \" + str(y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692bda86",
   "metadata": {},
   "source": [
    "Can you do better? Try tweaking the learning rate, the number of layers, and the neurons per layer to see if your validation loss improves. Once you have decided on your final model, evaluate it on the test set and note down your loss there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4243b595",
   "metadata": {},
   "source": [
    "# 2. Regularization\n",
    "\n",
    "We learned about a number of regularization techniques. Here, we will see how to implement early stopping, L2-regularization, and dropout-regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a2aeb7",
   "metadata": {},
   "source": [
    "## 2.1 Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e967ce4",
   "metadata": {},
   "source": [
    "When we implement early stopping, the model definition and compilation is unchanged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fb9122",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e2f299",
   "metadata": {},
   "source": [
    "However, we now need to add a so-called `callback` to the training process. We define the `EarlyStopping` callback, which interrupts training if the validation loss is no longer improving. In particular, the callback waits for `patience` epochs of no improvement before interrupting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a7a3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0271305c",
   "metadata": {},
   "source": [
    "The other parameter here is `restore_best_weights`. If set to `True`, this simply means that, once the callback decides to interrupt, it takes the version of the model that led to the best validation loss so far (do you know which epoch this corresponds to?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f953dff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = model.fit(X_train, y_train, epochs=100,\n",
    "                validation_data=(X_valid, y_valid),\n",
    "                callbacks=[early_stopping_cb])\n",
    "create_plot(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19e65d0",
   "metadata": {},
   "source": [
    "Now that we have stopped early, compare the performance of the model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d55021",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44d9086",
   "metadata": {},
   "source": [
    "## 2.2 L2- regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69c479a",
   "metadata": {},
   "source": [
    "L2-regularization adds a penalty based on the L2-norm to the loss function. Usually, we add the same penalty for all weights, and we don't add a penalty for the biases. But you could also add a `bias_regularizer`, or even an `activity_regularizer`, which regularizes the output of the neurons instead of the parameters.\n",
    "\n",
    "Keep in mind that the regularization parameter is another hyperparameter that might need tuning. A good starting point is 0.01, but it can vary quite a bit depending on the problem and network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a4131a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_param = 0.01\n",
    "regularizer = tf.keras.regularizers.l2(reg_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d614e6d8",
   "metadata": {},
   "source": [
    "Can you rerun the model from above, but using regularization? In particular, to each `Dense` layer, you want to add the argument `kernel_regularizer=regularizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4255db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\",kernel_regularizer=regularizer),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\",kernel_regularizer=regularizer),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\",kernel_regularizer=regularizer)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
    "              metrics=[\"accuracy\"])\n",
    "log = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "create_plot(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115ecc21",
   "metadata": {},
   "source": [
    "As the regularlization term is added to the loss, the loss will typically start out quite high, before the optimization routine finds a good way to adjust the weights to reduce the loss.\n",
    "\n",
    "Note that we have improved the overfitting issue quite a bit, but unfortunately we have made it more difficult for the model to learn (we introduced bias). This is frequently the case, and we usually need to do some fiddling to find a good compromise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c97cc8d",
   "metadata": {},
   "source": [
    "## 2.3 Dropout-regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ef8ffa",
   "metadata": {},
   "source": [
    "Another regularization method is dropout-regularization. At training time, in each iteration, a number of neurons will be considered as non-existent, so we force the network to distribute weights more equally across neurons.\n",
    "\n",
    "This makes the correct computation for activations a bit challenging when doing predictions, but luckily TensorFlow takes care of the added complexity.\n",
    "\n",
    "If we want to ensure that neurons at a certain layer drop out (with probability `rate`), we add a `Dropout` layer before the corresponding `Dense` layer, using\n",
    "```\n",
    "tf.keras.layers.Dropout(rate=0.2)\n",
    "```\n",
    "Of course, `0.2` is just a particular choice and we can vary that.\n",
    "Can you repeat the previous (baseline) model, but adding a `Dropout` layer before each `Dense` layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fbc9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
    "              metrics=[\"accuracy\"])\n",
    "log = model.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "create_plot(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0abad2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b569cfd",
   "metadata": {},
   "source": [
    "# 3. Vanishing / exploding gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ecfb2f",
   "metadata": {},
   "source": [
    "Below, we create a much deeper neural network. As you can see, not much is happening int erms of learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d918f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(100, activation=\"sigmoid\"),\n",
    "    tf.keras.layers.Dense(30, activation=\"sigmoid\"),\n",
    "    tf.keras.layers.Dense(30, activation=\"sigmoid\"),\n",
    "    tf.keras.layers.Dense(30, activation=\"sigmoid\"),\n",
    "    tf.keras.layers.Dense(30, activation=\"sigmoid\"),\n",
    "    tf.keras.layers.Dense(30, activation=\"sigmoid\"),\n",
    "    tf.keras.layers.Dense(30, activation=\"sigmoid\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
    "              metrics=[\"accuracy\"])\n",
    "log = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ea3832",
   "metadata": {},
   "source": [
    "## 3.1 Batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688c091f",
   "metadata": {},
   "source": [
    "Batch normalization allows us to do normalization at all stages of the network. For each input that is normalized, we need 4 parameters:\n",
    "1. One that determines how the input is scaled (trainable)\n",
    "1. One that determines how the input is shifted (trainable)\n",
    "1. One that keeps track of the average of that input (non-trainable - it is still being adjusted though!)\n",
    "1. One that keeps track of the standard deviation of that input (non-trainable - it is still being adjusted though!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8839fc",
   "metadata": {},
   "source": [
    "### Option 1: After activation (before inputs are weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b7d4f9",
   "metadata": {},
   "source": [
    "We can simply add a `BatchNormalization` layer before each of our `Dense` layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e99464",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(100, activation=\"sigmoid\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(30, activation=\"sigmoid\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(30, activation=\"sigmoid\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(30, activation=\"sigmoid\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(30, activation=\"sigmoid\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(30, activation=\"sigmoid\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(30, activation=\"sigmoid\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fad3280",
   "metadata": {},
   "source": [
    "Can you verify the number of trainable and non-trainable parameters?\n",
    "\n",
    "Let's now train the network again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79834149",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
    "              metrics=[\"accuracy\"])\n",
    "log = model.fit(X_train, y_train, epochs=5,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb6fa85",
   "metadata": {},
   "source": [
    "We can train even a deep neural network much more easily!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfb241c",
   "metadata": {},
   "source": [
    "### Option 2: Before activation (after inputs are weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356dfa62",
   "metadata": {},
   "source": [
    "The reommendation by the authors of the original paper on batch normalization is to normalize the weighted sum that goes into the neurons. That is, we first combine the inputs (and add a bias), then we \"normalize\" that weighted sum, before running the activation function on it. To do so in TensorFlow, we have to split apart our hidden layers into the combination and the activaiton. We out the `BatchNormalization` in-between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd4d061",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(100),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('sigmoid'),\n",
    "    tf.keras.layers.Dense(30),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('sigmoid'),\n",
    "    tf.keras.layers.Dense(30),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('sigmoid'),\n",
    "    tf.keras.layers.Dense(30),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('sigmoid'),\n",
    "    tf.keras.layers.Dense(30),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('sigmoid'),\n",
    "    tf.keras.layers.Dense(30),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('sigmoid'),\n",
    "    tf.keras.layers.Dense(30),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('sigmoid'),\n",
    "    tf.keras.layers.Dense(10),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed77320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
    "              metrics=[\"accuracy\"])\n",
    "log = model.fit(X_train, y_train, epochs=5,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff08640",
   "metadata": {},
   "source": [
    "In practice, the differences between the two options tend to be small. But if you are really struggling to get your network to learn anything, try it out like this!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9526df17",
   "metadata": {},
   "source": [
    "## 3.2 Specific initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea52caf",
   "metadata": {},
   "source": [
    "We generally want to initialize our weights in a sensible manner (especially if we are not using batch normalization, for example, because of runtime concerns). Let's start with our baseline model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55bb697",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(631)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "weights, biases = model.layers[1].get_weights()\n",
    "print(\"First layer: \" + str(weights[0,0]))\n",
    "weights, biases = model.layers[-1].get_weights()\n",
    "print(\"Last layer: \" + str(weights[0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a777278",
   "metadata": {},
   "source": [
    "Can you copy the model definition, but change the first layer to `kernel_initializer='he_normal'` and the output layer to `kernel_initializer='glorot_uniform'`?\n",
    "\n",
    "What changes to you observe in the first layer, what changes in the last layer? Do they make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0543d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(631)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\", kernel_initializer=\"glorot_uniform\")\n",
    "])\n",
    "\n",
    "weights, biases = model.layers[1].get_weights()\n",
    "print(\"First layer: \" + str(weights[0,0]))\n",
    "weights, biases = model.layers[-1].get_weights()\n",
    "print(\"Last layer: \" + str(weights[0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8007b36d",
   "metadata": {},
   "source": [
    "Try again. Can you copy the model definition, but change the first layer to `kernel_initializer='he_uniform'` and the output layer to `kernel_initializer='glorot_normal'`?\n",
    "\n",
    "What changes to you observe in the first layer, what changes in the last layer? Do they make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de81ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(631)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_uniform\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\", kernel_initializer=\"glorot_normal\")\n",
    "])\n",
    "\n",
    "weights, biases = model.layers[1].get_weights()\n",
    "print(\"First layer: \" + str(weights[0,0]))\n",
    "weights, biases = model.layers[-1].get_weights()\n",
    "print(\"Last layer: \" + str(weights[0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6fed4e",
   "metadata": {},
   "source": [
    "# 4. Speeding up learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70784ab8",
   "metadata": {},
   "source": [
    "## 4.1 Mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4addebe6",
   "metadata": {},
   "source": [
    "You might not have noticed, but we run mini-batch gradient descent by default. We can control the batch-size within the `model.fit` function. The default is `32`. Run the below code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be56928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "log = model.fit(X_train, y_train,\n",
    "                epochs=3,batch_size=32,\n",
    "                validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea126bf",
   "metadata": {},
   "source": [
    "Can you remake the model, but change the `batch_size` to `1024`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ea24c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "log = model.fit(X_train, y_train,\n",
    "                epochs=3, batch_size=1024,\n",
    "                validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c24a9f",
   "metadata": {},
   "source": [
    "Notice the number of steps taken in each epoch (the counter just underneath \"Epoch x/3\"). Can you explain where the number of steps are coming from?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75b5635",
   "metadata": {},
   "source": [
    "## 4.2 Using Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aedf58a",
   "metadata": {},
   "source": [
    "We can add momentum to many algorithms. The base case is to add momentum to `SGD`. A typical value is `0.9` but keep in mind that this is another hyperparameter that may need some tuning. When setting up `SGD`, you can also tick `nesterov=True` to use the Nesterov algorithm, a moment-based algorithm we didn't discuss. Try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924e4f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=False),\n",
    "              metrics=[\"accuracy\"])\n",
    "log = model.fit(X_train, y_train,epochs=20,\n",
    "                validation_data=(X_valid, y_valid))\n",
    "create_plot(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff246c4f",
   "metadata": {},
   "source": [
    "## 4.3 RMSpop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66485903",
   "metadata": {},
   "source": [
    "RMSprop is an algorithm that pursues a slightly different idea: it normalizes the gradients using their squares. It requires to specify a `learning_rate`, as well as the hyperparameters `rho` and `epsilon`. For the latter two, the standard values usually do just fine, while even the `learning_rate` is less problematic than in `SGD`.\n",
    "\n",
    "If you want, you can also add `momentum` to the algorithm.\n",
    "\n",
    "Try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e3afa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9, epsilon=1e-07, momentum=0.0),\n",
    "              metrics=[\"accuracy\"])\n",
    "log = model.fit(X_train, y_train,epochs=20,\n",
    "                validation_data=(X_valid, y_valid))\n",
    "create_plot(log)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9d2ffee",
   "metadata": {},
   "source": [
    "## 4.4 Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8686cc",
   "metadata": {},
   "source": [
    "Finally, we have `Adam`, which is used most commonly. Adam combines the ideas of RMSprop and momentum gradient descent. However, it also adds a slight adjustment that is particularly relevant for early iterations. The hyperparameters are `learning_rate`, `beta_1`, `beta_2`, and `epislon`, even though mostly people leave everything but the `learning_rate` alone. Try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f491ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07),\n",
    "              metrics=[\"accuracy\"])\n",
    "log = model.fit(X_train, y_train,epochs=20,\n",
    "                validation_data=(X_valid, y_valid))\n",
    "create_plot(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c24c24",
   "metadata": {},
   "source": [
    "# 5. Learning rate schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6344b7a4",
   "metadata": {},
   "source": [
    "## 5.1 Power scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc7ad82",
   "metadata": {},
   "source": [
    "Remember that each epoch contains a number of steps ($\\frac{n}{\\text{mini-batch-size}}$ to be exact). If we want to express our decay schedule based on the number of epochs, we first have to make a bit of an adjustment. For example, say that we specify the min-batch-size to 32 and that we want to have reach the next \"decay step\" (i.e., 1/2, 1/3, 1/4, ...) after 5 epochs.\n",
    "\n",
    "Can you define the correct `s`, which should be the number of steps (not epochs!) until we reach the next \"decay step\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbffb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs_until_change = 5\n",
    "\n",
    "steps_per_epoch = X_train.shape[0] / batch_size\n",
    "s = epochs_until_change * steps_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1cb53a",
   "metadata": {},
   "source": [
    "Once we have defined the right `s`, we can train the model by manually defining our optimizer, using the TensorFlow scheduling process.\n",
    "\n",
    "Here, we use `InverseTimeDecay` which computes\n",
    "```\n",
    "current_learning_rate = initial_learning_rate / (1 + decay_rate * step / decay_steps)\n",
    "```\n",
    "Increasing the `decay_rate` is equivalent to decreasing the `decay_steps`. Since we have already tuned `decay_steps=s`, we can simply set the `decay_rate` to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1343bf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "learning_rate = tf.keras.optimizers.schedules.InverseTimeDecay(initial_learning_rate=0.01, decay_steps=s, decay_rate=1)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "log = model.fit(X_train, y_train,\n",
    "                epochs=20,batch_size=batch_size,\n",
    "                validation_data=(X_valid, y_valid))\n",
    "create_plot(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049bb7cb",
   "metadata": {},
   "source": [
    "## 5.2 Exponential scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da73c2d4",
   "metadata": {},
   "source": [
    "When we use any other type of scheduling, we can follow the same process. In particular, can you redefine `s`, but this time with 10 epochs until we reach the next stage in the schedule?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db88d8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs_until_change = 10\n",
    "\n",
    "steps_per_epoch = X_train.shape[0] / batch_size\n",
    "s = epochs_until_change * steps_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1013f117",
   "metadata": {},
   "source": [
    "We now use the `ExponentialDecay` schedule. Here, the computation is\n",
    "```\n",
    "current_learning_rate = initial_learning_rate * decay_rate**(step / decay_steps)\n",
    "```\n",
    "(Note that `**` means to the power of)\n",
    "\n",
    "Our baseline exponential schedule has a `0.1` base, so we set `decay_rate=0.1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dec3de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.01, decay_steps=s, decay_rate=0.1)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "log = model.fit(X_train, y_train,\n",
    "                epochs=20,batch_size=batch_size,\n",
    "                validation_data=(X_valid, y_valid))\n",
    "create_plot(log)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adl_env",
   "language": "python",
   "name": "adl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
