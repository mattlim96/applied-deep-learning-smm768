{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b346f039",
      "metadata": {
        "id": "b346f039"
      },
      "source": [
        "# Part 1: CNNs and Transfer Learning in General"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89383c5c",
      "metadata": {
        "id": "89383c5c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.applications.mobilenet import MobileNet, preprocess_input\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, UpSampling2D, MaxPool2D\n",
        "from tensorflow.keras import backend as K\n",
        "from scipy.ndimage import zoom"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84acc61a",
      "metadata": {
        "id": "84acc61a"
      },
      "source": [
        "## Data\n",
        "\n",
        "We'll be using the [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html), which contains some natural images tagged in 10 different categories (e.g. cars, dogs, birds etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfea0d7a",
      "metadata": {
        "id": "dfea0d7a"
      },
      "outputs": [],
      "source": [
        "num_classes = 10 # number of classes in the data\n",
        "img_rows, img_cols, img_channels = 32, 32, 3 # input image dimensions\n",
        "\n",
        "# Load and convert data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# Convert class vectors to one-hot encoding\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# Depending on the implementation, the underlying libraries might want the image \n",
        "# dimensions in different orders, check for it and reshape\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_channels, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_channels, img_rows, img_cols)\n",
        "    input_shape = (img_channels, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, img_channels)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, img_channels)\n",
        "    input_shape = (img_rows, img_cols, img_channels)\n",
        "\n",
        "# To speed up things, we select 1k random samples for training and test\n",
        "index = np.arange(x_train.shape[0])\n",
        "np.random.seed(0)\n",
        "np.random.shuffle(index)\n",
        "index = index[:1000]\n",
        "x_train, y_train = x_train[index], y_train[index]\n",
        "\n",
        "index = np.arange(x_test.shape[0])\n",
        "np.random.seed(0)\n",
        "np.random.shuffle(index)\n",
        "index = index[:1000]\n",
        "x_test, y_test = x_test[index], y_test[index]\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('y_train shape:', y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48d91bf9",
      "metadata": {
        "id": "48d91bf9"
      },
      "source": [
        "## Exercise 1.1: Building a CNN from scratch\n",
        "\n",
        "Let's build a CNN from scratch using a very small subset of CIFAR10 training data (1000 data points)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb31e37e",
      "metadata": {
        "id": "eb31e37e"
      },
      "outputs": [],
      "source": [
        "x_scaled_train = x_train / 255.\n",
        "x_scaled_test = x_test / 255."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8927edd9",
      "metadata": {
        "id": "8927edd9"
      },
      "outputs": [],
      "source": [
        "model1 = Sequential([\n",
        "    Conv2D(16, kernel_size=(3, 3),strides=1, padding='valid',activation='relu',input_shape=input_shape),\n",
        "    Conv2D(16, kernel_size=(3, 3),strides=2, padding='valid',activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(num_classes, activation='softmax')])\n",
        "\n",
        "model1.compile(loss='categorical_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "              metrics=['accuracy'])\n",
        "model1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d960552",
      "metadata": {
        "id": "0d960552"
      },
      "outputs": [],
      "source": [
        "model1.fit(x_scaled_train, y_train,\n",
        "          batch_size=128,\n",
        "          epochs=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d27c6e79",
      "metadata": {
        "id": "d27c6e79"
      },
      "source": [
        "Finally, we evaluate the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b8a9c3d",
      "metadata": {
        "id": "0b8a9c3d"
      },
      "outputs": [],
      "source": [
        "train_score = model1.evaluate(x_scaled_train , y_train, verbose=0)\n",
        "test_score = model1.evaluate(x_scaled_test, y_test, verbose=0)\n",
        "\n",
        "print('Train loss:', train_score[0])\n",
        "print('Train accuracy:', train_score[1])\n",
        "print('Test loss:', test_score[0])\n",
        "print('Test accuracy:', test_score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "018831c3",
      "metadata": {
        "id": "018831c3"
      },
      "source": [
        "### Questions\n",
        "\n",
        "1. Why is there such a big difference between the training and testing accuracy?\n",
        "2. Why is the performance so poor?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "647695a2",
      "metadata": {
        "id": "647695a2"
      },
      "source": [
        "## Exercise 1.2: Using pre-processed features\n",
        "\n",
        "Let's load a pre-trained MobileNet network (efficient, small network, only 4M parameters but with decent performance on ImageNet) and extract features from our training and testsets.\n",
        "\n",
        "First, plot a sample image (so we can make sure our transformations are correct):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6eff7bb",
      "metadata": {
        "id": "a6eff7bb"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "plt.imshow(x_train[0].astype(int))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39536ec8",
      "metadata": {
        "id": "39536ec8"
      },
      "source": [
        "Some pre-proessing of the image for MobileNet (scaling and shifing to be between [-1, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "871c3234",
      "metadata": {
        "id": "871c3234"
      },
      "outputs": [],
      "source": [
        "x_preprocess_train = preprocess_input(x_train.copy())\n",
        "x_preprocess_test = preprocess_input(x_test.copy())\n",
        "x_preprocess_train.shape, x_preprocess_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7a0c236",
      "metadata": {
        "id": "f7a0c236"
      },
      "source": [
        "Replot sample image (need to shift/scale to dispaly colours properly):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "232ec3db",
      "metadata": {
        "id": "232ec3db"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "plt.imshow((x_preprocess_train[0] + 1.0) * 127.5 / 255)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bad3e262",
      "metadata": {
        "id": "bad3e262"
      },
      "source": [
        "We need to resize the image because MobileNet expects one of a certain number of image sizes, e.g., 160x160x3, or 224x224x3. One way to do so is to use the `zoom` function, which adds in new pixels whose value is based on spline interpolation (this might take a few minutes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "976b05d1",
      "metadata": {
        "id": "976b05d1"
      },
      "outputs": [],
      "source": [
        "print('Processing training data')\n",
        "resized_images = []\n",
        "for i in range(x_preprocess_train.shape[0]):\n",
        "    resized_images.append(zoom(x_preprocess_train[i], (5.0, 5.0, 1.0)))\n",
        "    if (i+1) % 200 == 0:\n",
        "        print('* Processed %d images' % (i+1))\n",
        "x_preprocess_train = np.stack(resized_images)\n",
        "\n",
        "print('Processing testing data')\n",
        "resized_images = []\n",
        "for i in range(x_preprocess_test.shape[0]):\n",
        "    resized_images.append(zoom(x_preprocess_test[i], (5.0, 5.0, 1.0)))\n",
        "    if (i+1) % 200 == 0:\n",
        "        print('* Processed %d images' % (i+1))\n",
        "x_preprocess_test = np.stack(resized_images)\n",
        "\n",
        "x_preprocess_train.shape, x_preprocess_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "200bcc0d",
      "metadata": {
        "id": "200bcc0d"
      },
      "source": [
        "Plot another sample image (needs to be reshaped appropriately):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "589be0a5",
      "metadata": {
        "id": "589be0a5"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "plt.imshow((x_preprocess_train[0] + 1.0) * 127.5 / 255)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99b03bd1",
      "metadata": {
        "id": "99b03bd1"
      },
      "source": [
        "We are now ready to build a model. Our basis will be the MobileNet network, with the parameters trained on ImageNet.  Notice we add the `include_top=False` parameter because we don't want to use the included ImageNet classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaa7c018",
      "metadata": {
        "id": "aaa7c018"
      },
      "outputs": [],
      "source": [
        "mobilenet = MobileNet(weights='imagenet', include_top=False, input_shape = (160,160,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d3913b7",
      "metadata": {
        "id": "2d3913b7"
      },
      "outputs": [],
      "source": [
        "mobilenet.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d89a4bc",
      "metadata": {
        "id": "0d89a4bc"
      },
      "source": [
        "MobileNet outputs a (5, 5, 1024) tensor before it's \"top layer\" (which we didn't download).  We compute the relevant activation of this top layer for our images (this represents a range of high-level features of our images). We then aggregate the top layer activation using a `GlobalAveragePooling2D` layer, which compresses it into a (1, 1, 1024) tensor, which is equivalent to a 1024-vector.  `GlobalAveragePooling2D` works by taking the average of each 5x5 feature map.\n",
        "\n",
        "Let's extract high-level features of our images using MobileNet (this may take a few minutes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eec75697",
      "metadata": {
        "id": "eec75697"
      },
      "outputs": [],
      "source": [
        "x_features_train = mobilenet.predict(x_preprocess_train)\n",
        "x_features_test = mobilenet.predict(x_preprocess_test)\n",
        "x_features_train.shape, x_features_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6439375",
      "metadata": {
        "id": "e6439375"
      },
      "source": [
        "Let's now build a simple feed-forward network which uses these features as an input, to predict the right classes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43fdcab3",
      "metadata": {
        "id": "43fdcab3"
      },
      "outputs": [],
      "source": [
        "model2 = Sequential([\n",
        "    GlobalAveragePooling2D(input_shape=(5,5,1024)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(num_classes, activation='softmax')])\n",
        "model2.compile(loss='categorical_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "              metrics=['accuracy'])\n",
        "model2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7403e938",
      "metadata": {
        "id": "7403e938"
      },
      "outputs": [],
      "source": [
        "model2.fit(x_features_train, y_train,\n",
        "           batch_size=128,\n",
        "           epochs=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6461bbbb",
      "metadata": {
        "id": "6461bbbb"
      },
      "source": [
        "Finally, we evaluate the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca62f643",
      "metadata": {
        "id": "ca62f643"
      },
      "outputs": [],
      "source": [
        "train_score = model2.evaluate(x_features_train, y_train, verbose=0)\n",
        "test_score = model2.evaluate(x_features_test, y_test, verbose=0)\n",
        "\n",
        "print('Train loss:', train_score[0])\n",
        "print('Train accuracy:', train_score[1])\n",
        "print('Test loss:', test_score[0])\n",
        "print('Test accuracy:', test_score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7650d6d6",
      "metadata": {
        "id": "7650d6d6"
      },
      "source": [
        "### Questions\n",
        "\n",
        "1. What happens if you don't use the `GlobalAveragePooling2D` layer (note: you will need a `Flatten` layer instead)?  Hypothesize why the performance changes.\n",
        "2. Why does the image need to be preprocessed via the `preprocess_input()` function?  Hypothesize what happens if we omit this step.\n",
        "3. Why do we need to resize the image (via `zoom`)?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d108e87b",
      "metadata": {
        "id": "d108e87b"
      },
      "source": [
        "## Exercise 1.3: Using Frozen Layers in a Pre-trained Network\n",
        "\n",
        "Instead of using pre-processed features, we add the pre-trained MobileNet network to our model (and freeze it, to avoid changing its weights)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6260fbd",
      "metadata": {
        "id": "e6260fbd"
      },
      "outputs": [],
      "source": [
        "x_preprocess_train = preprocess_input(x_train.copy())\n",
        "x_preprocess_test = preprocess_input(x_test.copy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "636bb4ba",
      "metadata": {
        "id": "636bb4ba"
      },
      "outputs": [],
      "source": [
        "mobilenet = MobileNet(weights='imagenet', include_top=False, input_shape = (160,160,3))\n",
        "mobilenet.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b39e8b3",
      "metadata": {
        "id": "2b39e8b3"
      },
      "source": [
        "We integrate the `MobileNet` layers into our network. But before that, we need to resize the images. This time, we use \"upsampling\". Basically, we add additional pixels that are copies of the existing ones, i.e., we \"stretch out\" the pixels.\n",
        "\n",
        "We will conduct upsampling directly in our neural network, so there is not need to do additional pre-processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db26a7e7",
      "metadata": {
        "id": "db26a7e7"
      },
      "outputs": [],
      "source": [
        "model3 = Sequential([\n",
        "    UpSampling2D(size=(5,5), input_shape = (32,32,3)),\n",
        "    mobilenet,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(num_classes, activation='softmax')])\n",
        "    \n",
        "model3.compile(loss='categorical_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "              metrics=['accuracy'])\n",
        "model3.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2d0ebfc",
      "metadata": {
        "id": "a2d0ebfc"
      },
      "source": [
        "We train the model, as before. However (unless you are using a GPU), this is a lot slower. Hence, I recommend running for only a few periods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b4a92c3",
      "metadata": {
        "id": "5b4a92c3"
      },
      "outputs": [],
      "source": [
        "model3.fit(x_preprocess_train, y_train,\n",
        "           batch_size=128,\n",
        "           epochs=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2456a51a",
      "metadata": {
        "id": "2456a51a"
      },
      "source": [
        "Once more, let's look at how the model performs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd4a8494",
      "metadata": {
        "id": "cd4a8494"
      },
      "outputs": [],
      "source": [
        "train_score = model3.evaluate(x_preprocess_train, y_train, verbose=0)\n",
        "test_score = model3.evaluate(x_preprocess_test, y_test, verbose=0)\n",
        "\n",
        "print('Train loss:', train_score[0])\n",
        "print('Train accuracy:', train_score[1])\n",
        "print('Test loss:', test_score[0])\n",
        "print('Test accuracy:', test_score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61ad5efc",
      "metadata": {
        "id": "61ad5efc"
      },
      "source": [
        "### Questions\n",
        "\n",
        "1. Why does the network train so much more slowly?\n",
        "2. What is the difference (if any) between the model in Exercise 2 and Exercise 3?\n",
        "3. How would you modify the above code if we wanted to train some of the MobileNet layers? Hint: you can iterate through the layers of a model with\n",
        "```\n",
        "for layer in model.layers:\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bfcc04b",
      "metadata": {
        "id": "2bfcc04b"
      },
      "source": [
        "# Part 2: Hot Dog or Not Dog"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f590abc1",
      "metadata": {
        "id": "f590abc1"
      },
      "source": [
        "This problem's purpose is to build a neural network to classify images as hot dogs or not-hot dogs. This is the same problem as seen in the HBO TV show \"Silicon Valley\". We will be using the dataset put together by [a user on Kaggle](https://www.kaggle.com/dansbecker/hot-dog-not-hot-dog) which contains 498 training images and 500 test images.\n",
        "\n",
        "A simple CNN is given below. Due to the small sample size it has a very poor test set accuracy. Your task is to build a CNN that can beat this test set accuracy by a large margin (get to at least 70% test set accuracy)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8080ba5",
      "metadata": {
        "id": "d8080ba5"
      },
      "source": [
        "First, we need a few more packages. If you don't currently have skimage or cv2 installed, uncomment and run the lines below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d48558e4",
      "metadata": {
        "id": "d48558e4"
      },
      "outputs": [],
      "source": [
        "#pip install scikit-image\n",
        "#pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4780a7b",
      "metadata": {
        "id": "d4780a7b"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os, sys\n",
        "import cv2\n",
        "import tarfile\n",
        "from skimage.transform import resize\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c4107e4",
      "metadata": {
        "id": "3c4107e4"
      },
      "source": [
        "We start by loading the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "748420a0",
      "metadata": {
        "id": "748420a0"
      },
      "outputs": [],
      "source": [
        "path_to_data =  tf.keras.utils.get_file('hotdog.tar', 'https://www.dropbox.com/s/9zx61bhlrjx135j/hotdog.tar?dl=1')\n",
        "file = tarfile.open(path_to_data)\n",
        "file.extractall(os.path.abspath(os.path.join(path_to_data, os.pardir)))\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b226e774",
      "metadata": {
        "id": "b226e774"
      },
      "source": [
        "Let's take a look at two examples pictures:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54cacabd",
      "metadata": {
        "id": "54cacabd"
      },
      "outputs": [],
      "source": [
        "img_size = 160\n",
        "\n",
        "img_arr_hotdog = cv2.imread(os.path.abspath(os.path.join(path_to_data, os.pardir,'hotdog/train/hot_dog/2417.jpg')))\n",
        "img_arr_hotdog = cv2.resize(img_arr_hotdog, (img_size,img_size))[:,:,::-1]\n",
        "\n",
        "img_arr_notdog = cv2.imread(os.path.abspath(os.path.join(path_to_data, os.pardir,'hotdog/train/not_hot_dog/197.jpg')))\n",
        "img_arr_notdog = cv2.resize(img_arr_notdog, (img_size,img_size))[:,:,::-1]\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(img_arr_hotdog)\n",
        "plt.title(\"Hot dog\"); plt.grid(False)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(img_arr_notdog)\n",
        "plt.title(\"Not dog\"); plt.grid(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90f69b4f",
      "metadata": {
        "id": "90f69b4f"
      },
      "source": [
        "Instead of loading all the data in advance, we create a data pipeline using an `ImageDataGenerator`. The generator will load in the data files as needed and perform two transformations:\n",
        "- Rescaling pixels to be between [0, 1]\n",
        "- Resizing images to be in `img_size`x`img_size` (160x160)\n",
        "\n",
        "During training for each batch, the images are read from disk on the fly, loaded into memory and then the transformations are applied."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22a213ce",
      "metadata": {
        "id": "22a213ce"
      },
      "outputs": [],
      "source": [
        "train_data_dir = os.path.abspath(os.path.join(path_to_data, os.pardir,'hotdog/train'))\n",
        "test_data_dir = os.path.abspath(os.path.join(path_to_data, os.pardir,'hotdog/test'))\n",
        "batch_size = 128\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "# Data parameters (DO NOT MODIFY)\n",
        "num_train_samples = 498\n",
        "num_test_samples = 500\n",
        "\n",
        "# Data generators (DO NOT MODIFY)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=(img_size, img_size),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_data_dir,\n",
        "    target_size=(img_size, img_size),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3773633f",
      "metadata": {
        "id": "3773633f"
      },
      "source": [
        "### Defining and running an initial model\n",
        "\n",
        "We define a starting model, which you will need to improve upon."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44c064ac",
      "metadata": {
        "id": "44c064ac"
      },
      "outputs": [],
      "source": [
        "model1 = Sequential([\n",
        "    Conv2D(32, (3, 3), padding='valid', activation='relu', input_shape=(img_size,img_size,3)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    \n",
        "    Conv2D(32, (3, 3), padding='valid',activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    \n",
        "    Conv2D(64, (3, 3), padding='valid', activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    \n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "    \n",
        "model1.compile(loss='binary_crossentropy',\n",
        "              metrics=['accuracy'], \n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n",
        "\n",
        "model1.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fb2e0ed",
      "metadata": {
        "id": "3fb2e0ed"
      },
      "source": [
        "When training the model, there is a small detail to consider: since we generate data on the fly, the training process doesn't know the total number of data points. Normally, in each epoch, we would take as many steps as needed to get through the whole dataset, given our batch_size. Hence, we have \"training samples\" / \"batch size\" as the number of steps per epoch. Here, we have to manually define that number of steps instead (and we do it in exactly this way, for consistency of the meaning of \"epoch\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08cdef77",
      "metadata": {
        "id": "08cdef77"
      },
      "outputs": [],
      "source": [
        "model1.fit(train_generator,\n",
        "            steps_per_epoch=num_train_samples // batch_size,\n",
        "            epochs=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faeb5e9c",
      "metadata": {
        "id": "faeb5e9c"
      },
      "source": [
        "As usual, we evaluate the model. Again, the use of a generator implies only a small change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a80281b2",
      "metadata": {
        "id": "a80281b2"
      },
      "outputs": [],
      "source": [
        "train_score = model1.evaluate(train_generator,steps=num_test_samples // batch_size, verbose=0)\n",
        "test_score = model1.evaluate(test_generator,steps=num_test_samples // batch_size, verbose=0)\n",
        "\n",
        "print('Train loss:', train_score[0])\n",
        "print('Train accuracy:', train_score[1])\n",
        "print('Test loss:', test_score[0])\n",
        "print('Test accuracy:', test_score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bd5a7cc",
      "metadata": {
        "id": "6bd5a7cc"
      },
      "source": [
        "### Questions\n",
        "\n",
        "1. Can you improve the model using Transfer Learning? You could use the MobileNet as before, or some completely different pre-trained model, such as one of the different [ResNet](https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet) implementations in TensorFlow.\n",
        "2. Are there other things you can do to improve the model?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2cbac18",
      "metadata": {
        "id": "f2cbac18"
      },
      "source": [
        "### Example answer, part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7ed038f",
      "metadata": {
        "id": "f7ed038f"
      },
      "source": [
        "Using transfer learning as the training dataset is relatively small for the application (here: `MobileNet`)\n",
        "\n",
        "Since hotdog images probably have the same low-level details as that of the `ImageNet` dataset, such transfer learning should be feasible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa013e60",
      "metadata": {
        "id": "aa013e60"
      },
      "outputs": [],
      "source": [
        "mobilenet = MobileNet(weights='imagenet', include_top=False, input_shape = (160,160,3))\n",
        "mobilenet.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "341f26e0",
      "metadata": {
        "id": "341f26e0"
      },
      "outputs": [],
      "source": [
        "model2 = Sequential([\n",
        "    mobilenet,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model2.compile(loss='binary_crossentropy',\n",
        "              metrics=['accuracy'], \n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n",
        "\n",
        "model2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "484db9c4",
      "metadata": {
        "id": "484db9c4"
      },
      "outputs": [],
      "source": [
        "model2.fit(train_generator,\n",
        "            steps_per_epoch=num_train_samples // batch_size,\n",
        "            epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aefa7f8f",
      "metadata": {
        "id": "aefa7f8f"
      },
      "outputs": [],
      "source": [
        "train_score = model2.evaluate(train_generator,steps=num_test_samples // batch_size, verbose=0)\n",
        "test_score = model2.evaluate(test_generator,steps=num_test_samples // batch_size, verbose=0)\n",
        "\n",
        "print('Train loss:', train_score[0])\n",
        "print('Train accuracy:', train_score[1])\n",
        "print('Test loss:', test_score[0])\n",
        "print('Test accuracy:', test_score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "822a7bb0",
      "metadata": {
        "id": "822a7bb0"
      },
      "source": [
        "Compared to the original model, which contains 1.2 million parameters to train on 498 images, the transfer learning model contains 3.2 million parameters, but only 65.7 thousand are trainable. As a result, the performance improves significantly (even though there is quite a bit of overfitting happening, which we may want to address in a second step)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a24cfe5",
      "metadata": {
        "id": "9a24cfe5"
      },
      "source": [
        "### Example answer, part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8006173",
      "metadata": {
        "id": "a8006173"
      },
      "source": [
        "We can try to unfreeze some layers close to the top of pre-trained `MobileNet`, to adapt the representation more closely to our hotdog/notdog images. In particular, we only freeze the layers up to the second-to-last, and keep the last two unfrozen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a468745",
      "metadata": {
        "id": "7a468745"
      },
      "outputs": [],
      "source": [
        "mobilenet.trainable = True\n",
        "# Freeze layers in the base model except the last two\n",
        "for layer in mobilenet.layers[:-2]:\n",
        "    layer.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87807464",
      "metadata": {
        "id": "87807464"
      },
      "source": [
        "The model can be defined as before. Notice how we now have around 2,000 more parameters that are trainable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7392ea39",
      "metadata": {
        "id": "7392ea39"
      },
      "outputs": [],
      "source": [
        "model3 = Sequential([\n",
        "    mobilenet,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model3.compile(loss='binary_crossentropy',\n",
        "              metrics=['accuracy'], \n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n",
        "\n",
        "model3.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43839d1c",
      "metadata": {
        "id": "43839d1c"
      },
      "outputs": [],
      "source": [
        "model3.fit(train_generator,\n",
        "            steps_per_epoch=num_train_samples // batch_size,\n",
        "            epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b568092",
      "metadata": {
        "id": "4b568092"
      },
      "outputs": [],
      "source": [
        "train_score = model3.evaluate(train_generator,steps=num_test_samples // batch_size, verbose=0)\n",
        "test_score = model3.evaluate(test_generator,steps=num_test_samples // batch_size, verbose=0)\n",
        "\n",
        "print('Train loss:', train_score[0])\n",
        "print('Train accuracy:', train_score[1])\n",
        "print('Test loss:', test_score[0])\n",
        "print('Test accuracy:', test_score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15e103b5",
      "metadata": {
        "id": "15e103b5"
      },
      "source": [
        "By unfreezing layers close to the top, more parameters have become trainable. This can be helpful in training, but we don't see a vast improvement in this case (if any)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Non-sequential models\n",
        "\n",
        "We will now take a first peek at running non-sequential models. An example application is object detection (we need to predict classes **and** bounding boxes). But there are many other sceneraios where you want to tweak your model non-sequentially, e.g., to introduce skip connection (see the videos).\n",
        "\n",
        "Here, we will train a model that predicts both the class of an images, as well as a random number (this doesn't have much meaning but it is really just to show you how we can use the Functional API of TensorFlow).\n",
        "\n",
        "Let's first create the secondary y's (we will use the data from the first part):"
      ],
      "metadata": {
        "id": "2Ej3xy-kBCV8"
      },
      "id": "2Ej3xy-kBCV8"
    },
    {
      "cell_type": "code",
      "source": [
        "train_means = np.mean(x_preprocess_train,axis=(1,2,3))\n",
        "y2_train = np.random.normal(train_means,np.abs(train_means/2))\n",
        "\n",
        "test_means = np.mean(x_preprocess_test,axis=(1,2,3))\n",
        "y2_test = np.random.normal(test_means,np.abs(test_means/2))"
      ],
      "metadata": {
        "id": "xvqn69gog_9k"
      },
      "id": "xvqn69gog_9k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Functional API works very similarly to the Sequential API. But instead of having a list of layers, we just create layers and connect them arbitrarily. To do so, we just specify the previous layer that is supposed to flow into the current layer:"
      ],
      "metadata": {
        "id": "wQTAtcHohxjJ"
      },
      "id": "wQTAtcHohxjJ"
    },
    {
      "cell_type": "code",
      "source": [
        "model_input = Input(shape=(32, 32, 3)) # We start with an input layer (we could have multiple inputs, too!)\n",
        "\n",
        "x = Conv2D(32, kernel_size=(3,3), strides=(2,2), padding='same', activation=\"relu\")(model_input) # We then create a Convolutional layer, which takes the input layer as an input\n",
        "x = MaxPool2D((2,2), strides=(2,2), padding='same')(x) # Next, we create a Pooling layer that takes the convolutional layer as its input\n",
        "    \n",
        "x = Conv2D(64, kernel_size=(3,3), strides=(2,2), padding='same', activation=\"relu\")(x) # As before\n",
        "x = MaxPool2D((2,2), strides=(2,2), padding='same')(x) # As before\n",
        "     \n",
        "x = Flatten()(x)  # As before\n",
        "x = Dense(100,activation=\"relu\")(x)  # As before\n",
        "\n",
        "model_output_1 = Dense(num_classes, activation='softmax', name = 'output_1')(x) # Now we create an output layer that predicts the class (normal / pneumonia). It uses whatever comes out of the network so far\n",
        "model_output_2 = Dense(1, activation=\"sigmoid\", name = 'output_2')(x) # We create a second output layer. Note that this does not connect to the other output layer, but directly to the last hidden layer"
      ],
      "metadata": {
        "id": "9bivkln6gui6"
      },
      "id": "9bivkln6gui6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have all the same layers defined as before, just with a second output layer. Note that we give a specific name to our output-layers, so we can reference them later!\n",
        "\n",
        "We combine our layers in a model. We just have ot specify what our inputs are and what our outputs are. The remaining layers are added based on the structure above!"
      ],
      "metadata": {
        "id": "gCSjv6yRiPpv"
      },
      "id": "gCSjv6yRiPpv"
    },
    {
      "cell_type": "code",
      "source": [
        "model_func = Model(inputs = [model_input], outputs=[model_output_1, model_output_2])"
      ],
      "metadata": {
        "id": "wGqwUm_CiNtk"
      },
      "id": "wGqwUm_CiNtk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "See for yourself:"
      ],
      "metadata": {
        "id": "JzFN6oUviTVH"
      },
      "id": "JzFN6oUviTVH"
    },
    {
      "cell_type": "code",
      "source": [
        "model_func.summary()"
      ],
      "metadata": {
        "id": "fvN_PWZTiSL1"
      },
      "id": "fvN_PWZTiSL1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now compile our non-sequential model. We need to define our losses and metrics for each of our outputs! This is why we gave the output layers specific names, so we can use this here. The rest is as before:"
      ],
      "metadata": {
        "id": "P8FpdOn6iW_m"
      },
      "id": "P8FpdOn6iW_m"
    },
    {
      "cell_type": "code",
      "source": [
        "model_func.compile(loss={'output_1':'binary_crossentropy',\n",
        "                         'output_2':'mean_squared_error'},\n",
        "                   loss_weights = [1,0.01],\n",
        "                   metrics = {'output_1':'accuracy',\n",
        "                             'output_2':'mean_squared_error'},\n",
        "                   optimizer=tf.keras.optimizers.Adam(5e-4))"
      ],
      "metadata": {
        "id": "AwxnoVxyiUyU"
      },
      "id": "AwxnoVxyiUyU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar for fitting the model: We have to make clear that there are two different y values that need to be predicted. We will run the model only for a few epochs, to see how it works."
      ],
      "metadata": {
        "id": "HEIMzNe2iZf5"
      },
      "id": "HEIMzNe2iZf5"
    },
    {
      "cell_type": "code",
      "source": [
        "log_func = model_func.fit(x=x_preprocess_train,y=[y_train,y2_train],epochs=20)"
      ],
      "metadata": {
        "id": "c0pcC9eZiYIE"
      },
      "id": "c0pcC9eZiYIE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As always, we can evaluate the model:"
      ],
      "metadata": {
        "id": "Gi396UCzi1ct"
      },
      "id": "Gi396UCzi1ct"
    },
    {
      "cell_type": "code",
      "source": [
        "model_func.evaluate(x_preprocess_test, [y_test,y2_test])"
      ],
      "metadata": {
        "id": "Fj_nfUTtirN3"
      },
      "id": "Fj_nfUTtirN3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we can make predictions. We just have to note that two outputs are being predicted, the labels and the average values. But we can simply use list-indices to get the right ones."
      ],
      "metadata": {
        "id": "zXiId6bsjC-m"
      },
      "id": "zXiId6bsjC-m"
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model_func.predict(x_preprocess_test)"
      ],
      "metadata": {
        "id": "4Olz11CYi2v2"
      },
      "id": "4Olz11CYi2v2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions[0].shape"
      ],
      "metadata": {
        "id": "yknrTTJ-jP78"
      },
      "id": "yknrTTJ-jP78",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions[1].shape"
      ],
      "metadata": {
        "id": "R4QcBzW9jRR8"
      },
      "id": "R4QcBzW9jRR8",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "adl_env",
      "language": "python",
      "name": "adl_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "colab": {
      "name": "ADL_Week 8_CNNs and Transfer Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "7650d6d6",
        "61ad5efc",
        "6bd5a7cc"
      ]
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}